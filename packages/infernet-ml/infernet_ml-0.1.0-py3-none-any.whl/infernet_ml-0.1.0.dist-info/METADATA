Metadata-Version: 2.1
Name: infernet_ml
Version: 0.1.0
Summary: Lightweight library to build web3 machine learning workflows
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: <3.12,>=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: python-dotenv <2.0.0,>=1.0.0
Requires-Dist: pydantic <3.0.0,>=2.5.3
Requires-Dist: huggingface-hub <1.0.0,>=0.17.3
Requires-Dist: click <9.0.0,>=8.1.7
Provides-Extra: bark_inference
Requires-Dist: torch <3.0.0,>=2.1.2 ; extra == 'bark_inference'
Requires-Dist: transformers <5.0.0,>=4.37.2 ; extra == 'bark_inference'
Requires-Dist: scipy <2.0.0,>=1.11.4 ; extra == 'bark_inference'
Provides-Extra: css_inference
Requires-Dist: text-generation <1.0.0,>=0.6.1 ; extra == 'css_inference'
Requires-Dist: retry2 <1.0.0,>=0.9.5 ; extra == 'css_inference'
Provides-Extra: onnx_inference
Requires-Dist: torch <3.0.0,>=2.1.2 ; extra == 'onnx_inference'
Requires-Dist: onnx <2.0.0,>=1.15.0 ; extra == 'onnx_inference'
Requires-Dist: onnxruntime <2.0.0,>=1.16.3 ; extra == 'onnx_inference'
Provides-Extra: tgi_inference
Requires-Dist: text-generation <1.0.0,>=0.6.1 ; extra == 'tgi_inference'
Requires-Dist: retry2 <1.0.0,>=0.9.5 ; extra == 'tgi_inference'
Provides-Extra: torch_inference
Requires-Dist: torch <3.0.0,>=2.1.2 ; extra == 'torch_inference'
Requires-Dist: sk2torch <2.0.0,>=1.2.0 ; extra == 'torch_inference'

# infernet-ml

`infernet-ml` is a lightweight library meant to simplify the implementation
of machine learning workflows for models intended for Web3.

# Installation

### Via `pip`

To install this library via pip:

```bash
python -m venv .venv
source .venv/bin/activate
pip install "infernet-ml"
```

### Via `uv`

Alternatively, via [uv](https://github.com/astral-sh/uv):

```bash
uv venv
source .venv/bin/activate
uv pip install infernet-ml
```

## Optional Dependencies

Depending on the workflow you're using, you may want to install optional dependencies. For example, if you're using the
`torch` workflow, you'll need to install its dependencies by running:

```bash
pip install "infernet-ml[torch_inference]"
```

Alternatively, via [uv](https://github.com/astral-sh/uv):

```bash
uv pip install "infernet-ml[torch_inference]"
```

> [!NOTE] The optional dependencies for this workflow require that `cmake` is installed on your system. You can install
`cmake` on MacOS by running `brew install cmake`. On Ubuntu & Windows,
> consult [the documentation](https://onnxruntime.ai/docs/build/inferencing.html#prerequisites)
> for more information.

## Docs

For more information on this library, consult
the [documentation website](https://docs.ritual.net/ml-workflows/overview).
