# the workflow class to load. for example, you can change this to "infernet_ml.workflows.inference.css_inference_workflow.CSSInferenceWorkflow"
LLM_INF_WORKFLOW_CLASS="infernet_ml.workflows.inference.llm_inference_workflow.TGIClientInferenceWorkflow"

# first arg is tgi(Text Generation Interface) service url, second arg is connection timeout
LLM_INF_WORKFLOW_POSITIONAL_ARGS=["http://FILL_HOSTNAME_HERE", 30 ]

# Any arg passed here will be defaulted when sending to tgi
LLM_INF_WORKFLOW_KW_ARGS={}

# Workflow specific params below:
# Parameters to control tgi inference workflow retry logic

# number of retries
TGI_REQUEST_TRIES=3

# delays between retries in seconds
TGI_REQUEST_DELAY=3

# max delay between retry in seconds
TGI_REQUEST_MAX_DELAY=10

# backoff between retry in seconds
TGI_REQUEST_BACKOFF=2

# jitter to add to requests in seconds
TGI_REQUEST_JITTER=[0.5,1.5]
