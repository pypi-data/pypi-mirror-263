{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasbaldwin/opt/miniconda3/envs/coflows/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#imports\n",
    "from aiflows.utils.general_helpers import read_yaml_file, quick_load_api_keys\n",
    "from aiflows.utils import serve_utils\n",
    "from aiflows.utils import colink_utils\n",
    "from aiflows.workers import run_dispatch_worker_thread\n",
    "from aiflows.base_flows import AtomicFlow\n",
    "from aiflows.messages import FlowMessage\n",
    "from aiflows import flow_verse\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "from utils import compile_and_writefile, dict_to_yaml\n",
    "import json\n",
    "import copy\n",
    "#Specify path of your flow modules\n",
    "FLOW_MODULES_PATH = \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to the CoLink Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = colink_utils.start_colink_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ChatFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing ChatFlow From the FlowVerse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2024-03-18 16:43:39,963\u001b[0m][\u001b[34maiflows.flow_verse.loading:775\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m[<interactive>]\u001b[0m started to sync flow module dependencies to /Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/AMLD/ChatFlow/flow_modules...\u001b[0m\n",
      "[\u001b[36m2024-03-18 16:43:40,391\u001b[0m][\u001b[34maiflows.flow_verse.loading:608\u001b[0m][\u001b[32mINFO\u001b[0m] - aiflows/ChatFlowModule:coflows already synced, skip\u001b[0m\n",
      "[\u001b[36m2024-03-18 16:43:40,393\u001b[0m][\u001b[34maiflows.flow_verse.loading:825\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m[<interactive>]\u001b[0m finished syncing\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/AMLD/ChatFlow/flow_modules/aiflows/ChatFlowModule']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the ChatFlowModule from the flowverse on hugging face\n",
    "# link: https://huggingface.co/aiflows/ChatFlowModule/tree/coflows\n",
    "dependencies = [\n",
    "    {\"url\": \"aiflows/ChatFlowModule\", \"revision\": \"coflows\"},\n",
    "]\n",
    "#Dowload flow modules in \"./flow_modules\"\n",
    "flow_verse.sync_dependencies(dependencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"_target_\": \"flow_modules.aiflows.ChatFlowModule.ChatAtomicFlow.instantiate_from_default_config\",\n",
      "    \"name\": \"ChatAtomicFlow\",\n",
      "    \"description\": \"Flow which uses as tool an LLM though an API\",\n",
      "    \"enable_cache\": true,\n",
      "    \"n_api_retries\": 6,\n",
      "    \"wait_time_between_retries\": 20,\n",
      "    \"system_name\": \"system\",\n",
      "    \"user_name\": \"user\",\n",
      "    \"assistant_name\": \"assistant\",\n",
      "    \"backend\": {\n",
      "        \"_target_\": \"aiflows.backends.llm_lite.LiteLLMBackend\",\n",
      "        \"api_infos\": \"???\",\n",
      "        \"model_name\": \"gpt-3.5-turbo\",\n",
      "        \"n\": 1,\n",
      "        \"max_tokens\": 2000,\n",
      "        \"temperature\": 0.3,\n",
      "        \"top_p\": 0.2,\n",
      "        \"frequency_penalty\": 0,\n",
      "        \"presence_penalty\": 0,\n",
      "        \"stream\": true\n",
      "    },\n",
      "    \"system_message_prompt_template\": {\n",
      "        \"_target_\": \"aiflows.prompt_template.JinjaPrompt\"\n",
      "    },\n",
      "    \"init_human_message_prompt_template\": {\n",
      "        \"_target_\": \"aiflows.prompt_template.JinjaPrompt\"\n",
      "    },\n",
      "    \"human_message_prompt_template\": {\n",
      "        \"_target_\": \"aiflows.prompt_template.JinjaPrompt\",\n",
      "        \"template\": \"{{query}}\",\n",
      "        \"input_variables\": [\n",
      "            \"query\"\n",
      "        ]\n",
      "    },\n",
      "    \"input_interface_initialized\": [\n",
      "        \"query\"\n",
      "    ],\n",
      "    \"query_message_prompt_template\": {\n",
      "        \"_target_\": \"aiflows.prompt_template.JinjaPrompt\"\n",
      "    },\n",
      "    \"previous_messages\": {\n",
      "        \"first_k\": null,\n",
      "        \"last_k\": null\n",
      "    },\n",
      "    \"output_interface\": [\n",
      "        \"api_output\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Print the configuration\n",
    "default_cfg = read_yaml_file(\"flow_modules/aiflows/ChatFlowModule/ChatAtomicFlow.yaml\")\n",
    "print(json.dumps(default_cfg, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Serving & Getting and Instance of the ChatFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Serving the ChatFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started serving flow_modules.aiflows.ChatFlowModule.ChatAtomicFlow at flows:Chat Flow.\n",
      "dispatch_point: coflows_dispatch\n",
      "parallel_dispatch: False\n",
      "singleton: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve_utils.serve_flow(\n",
    "    cl=cl,\n",
    "    flow_class_name=\"flow_modules.aiflows.ChatFlowModule.ChatAtomicFlow\",\n",
    "    flow_endpoint=\"Chat Flow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Getting 2 Instances of the ChatFlow: One who answers in English and One who answers in French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch worker started in attached thread.\n",
      "dispatch_point: coflows_dispatch\n"
     ]
    }
   ],
   "source": [
    "#Â Start a worker thread to handle incoming messages\n",
    "run_dispatch_worker_thread(cl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"_target_\": \"flow_modules.aiflows.ChatFlowModule.ChatAtomicFlow.instantiate_from_default_config\",\n",
      "    \"name\": \"SimpleQA_Flow\",\n",
      "    \"description\": \"A flow that answers questions.\",\n",
      "    \"input_interface_non_initialized\": [\n",
      "        \"question\"\n",
      "    ],\n",
      "    \"backend\": {\n",
      "        \"_target_\": \"aiflows.backends.llm_lite.LiteLLMBackend\",\n",
      "        \"api_infos\": \"???\",\n",
      "        \"model_name\": {\n",
      "            \"openai\": \"gpt-3.5-turbo\",\n",
      "            \"azure\": \"azure/gpt-4\"\n",
      "        },\n",
      "        \"n\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 0.3,\n",
      "        \"top_p\": 0.2,\n",
      "        \"frequency_penalty\": 0,\n",
      "        \"presence_penalty\": 0\n",
      "    },\n",
      "    \"n_api_retries\": 6,\n",
      "    \"wait_time_between_retries\": 20,\n",
      "    \"system_message_prompt_template\": {\n",
      "        \"_target_\": \"aiflows.prompt_template.JinjaPrompt\",\n",
      "        \"template\": \"You are a helpful chatbot that truthfully answers questions.\",\n",
      "        \"input_variables\": [],\n",
      "        \"partial_variables\": {}\n",
      "    },\n",
      "    \"init_human_message_prompt_template\": {\n",
      "        \"_target_\": \"aiflows.prompt_template.JinjaPrompt\",\n",
      "        \"template\": \"Answer the following question: {{question}}\",\n",
      "        \"input_variables\": [\n",
      "            \"question\"\n",
      "        ],\n",
      "        \"partial_variables\": {}\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#get the demo config as a starting point\n",
    "root_dir = \".\"\n",
    "cfg_path = os.path.join(root_dir, \"flow_modules/aiflows/ChatFlowModule/demo.yaml\")\n",
    "cfg = read_yaml_file(cfg_path)\n",
    "print(json.dumps(cfg, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted 772025d8-d7d4-4e61-a2f2-3623152f7354 at flows:Chat Flow:mounts:local:772025d8-d7d4-4e61-a2f2-3623152f7354\n",
      "Mounted f5ac3956-421d-46a4-bd53-26036af29224 at flows:Chat Flow:mounts:local:f5ac3956-421d-46a4-bd53-26036af29224\n"
     ]
    }
   ],
   "source": [
    "#Get an instance of a the ChatFlow who speaks English\n",
    "from aiflows.backends.api_info import ApiInfo\n",
    "english_chatbot_overrides = copy.deepcopy(cfg)\n",
    "\n",
    "api_info = [ApiInfo(backend_used=\"openai\", api_key=os.getenv(\"OPENAI_API_KEY\"))]\n",
    "quick_load_api_keys(english_chatbot_overrides, api_info)\n",
    "\n",
    "english_chatbot_overrides[\"system_message_prompt_template\"][\"template\"] = \\\n",
    "\"You are a helpful chatbot that truthfully answers questions. Answer in the following language: English.\"\n",
    "\n",
    "english_chatbot = serve_utils.get_flow_instance(\n",
    "    cl=cl,\n",
    "    flow_endpoint=\"Chat Flow\",\n",
    "    config_overrides=english_chatbot_overrides,\n",
    ")\n",
    "\n",
    "#Get an instance of a the ChatFlow who speaks French\n",
    "french_chatbot_overrides = copy.deepcopy(cfg)\n",
    "api_info = [ApiInfo(backend_used=\"openai\", api_key=os.getenv(\"OPENAI_API_KEY\"))]\n",
    "quick_load_api_keys(french_chatbot_overrides, api_info)\n",
    "french_chatbot_overrides[\"system_message_prompt_template\"][\"template\"] = \\\n",
    "\"You are a helpful chatbot that truthfully answers questions. Answer in the following language: French.\"\n",
    "\n",
    "french_chatbot = serve_utils.get_flow_instance(\n",
    "    cl=cl,\n",
    "    flow_endpoint=\"Chat Flow\",\n",
    "    config_overrides=french_chatbot_overrides,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Calling the ChatFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Chatting with the English ChatFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'question': 'What is the capital of Switzerland?'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: 772025d8-d7d4-4e61-a2f2-3623152f7354\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:4042fc7a-b7ec-4e08-b288-b6d5dbac753b:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n",
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: 772025d8-d7d4-4e61-a2f2-3623152f7354\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:6131aafa-3252-488c-9466-f8986202d751:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n",
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: f5ac3956-421d-46a4-bd53-26036af29224\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:ebe5e1a0-bd1b-4403-baa4-f50330a86bc2:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n",
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: f5ac3956-421d-46a4-bd53-26036af29224\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:25aa6e1f-9366-4639-b2ed-e44d6e16ad16:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n",
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: ce9fc95d-6c90-46b1-bea2-f3275de2cb16\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:9effc98d-1d3d-4651-a41d-0ba9d84b0750:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n",
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: ce9fc95d-6c90-46b1-bea2-f3275de2cb16\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:076ce630-d9e9-4ee5-b806-48ffc1860eb7:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n",
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: 5e40a0dc-e543-446e-b600-75ca0f1cfcb3\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:64da74fd-4728-4566-b814-cfadd18c916e:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n",
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: 5e40a0dc-e543-446e-b600-75ca0f1cfcb3\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:3fe70e12-79a7-4c56-8f90-81c599bd0c77:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n",
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: 5e40a0dc-e543-446e-b600-75ca0f1cfcb3\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:57f0235a-cde5-4a29-8cb3-6cbfd8b662b3:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n",
      "[\u001b[36m2024-03-18 16:46:05,606\u001b[0m][\u001b[34maiflows.utils.general_helpers:387\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[1m\u001b[31m \n",
      "\n",
      "For feedback or to get help:  \u001b[0m\u001b[1m\u001b[32mhttps://github.com/epfl-dlab/aiflows/issues \n",
      "\n",
      "\u001b[0m\n",
      "[\u001b[36m2024-03-18 16:46:05,609\u001b[0m][\u001b[34maiflows.utils.general_helpers:397\u001b[0m][\u001b[31mERROR\u001b[0m] - 'query'\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/aiflows/utils/general_helpers.py\", line 406, in wrapper\n",
      "    return f(*args, **kw)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/aiflows/base_flows/abstract.py\", line 536, in __call__\n",
      "    self._run_method(input_message)\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/aiflows/base_flows/abstract.py\", line 521, in _run_method\n",
      "    self.run(input_message)\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/AMLD/ChatFlow/flow_modules/aiflows/ChatFlowModule/ChatAtomicFlow.py\", line 394, in run\n",
      "    response = self.query_llm(input_data=input_data)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/AMLD/ChatFlow/flow_modules/aiflows/ChatFlowModule/ChatAtomicFlow.py\", line 372, in query_llm\n",
      "    self._process_input(input_data)\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/AMLD/ChatFlow/flow_modules/aiflows/ChatFlowModule/ChatAtomicFlow.py\", line 347, in _process_input\n",
      "    user_message_content = self._get_message(self.human_message_prompt_template, input_data)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/AMLD/ChatFlow/flow_modules/aiflows/ChatFlowModule/ChatAtomicFlow.py\", line 219, in _get_message\n",
      "    template_kwargs[input_variable] = input_data[input_variable]\n",
      "                                      ~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'query'\n",
      "Error executing flow: 'query'\n",
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: b6886db5-5474-481d-84a3-0bc2da00f86e\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:2f34dd40-64f8-4831-a725-4e0cf8a3a36a:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n",
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: b6886db5-5474-481d-84a3-0bc2da00f86e\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:cc63504c-b297-411b-a16e-c9708d11c80b:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n",
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: b6886db5-5474-481d-84a3-0bc2da00f86e\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:6f9c604c-807b-44c0-bcbb-b50ffe698fdd:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n",
      "[\u001b[36m2024-03-18 16:47:21,975\u001b[0m][\u001b[34maiflows.utils.general_helpers:387\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[1m\u001b[31m \n",
      "\n",
      "For feedback or to get help:  \u001b[0m\u001b[1m\u001b[32mhttps://github.com/epfl-dlab/aiflows/issues \n",
      "\n",
      "\u001b[0m\n",
      "[\u001b[36m2024-03-18 16:47:21,984\u001b[0m][\u001b[34maiflows.utils.general_helpers:397\u001b[0m][\u001b[31mERROR\u001b[0m] - 'query'\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/aiflows/utils/general_helpers.py\", line 406, in wrapper\n",
      "    return f(*args, **kw)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/aiflows/base_flows/abstract.py\", line 536, in __call__\n",
      "    self._run_method(input_message)\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/aiflows/base_flows/abstract.py\", line 521, in _run_method\n",
      "    self.run(input_message)\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/AMLD/ChatFlow/flow_modules/aiflows/ChatFlowModule/ChatAtomicFlow.py\", line 394, in run\n",
      "    response = self.query_llm(input_data=input_data)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/AMLD/ChatFlow/flow_modules/aiflows/ChatFlowModule/ChatAtomicFlow.py\", line 372, in query_llm\n",
      "    self._process_input(input_data)\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/AMLD/ChatFlow/flow_modules/aiflows/ChatFlowModule/ChatAtomicFlow.py\", line 347, in _process_input\n",
      "    user_message_content = self._get_message(self.human_message_prompt_template, input_data)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/AMLD/ChatFlow/flow_modules/aiflows/ChatFlowModule/ChatAtomicFlow.py\", line 219, in _get_message\n",
      "    template_kwargs[input_variable] = input_data[input_variable]\n",
      "                                      ~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'query'\n",
      "Error executing flow: 'query'\n",
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: a54b486b-8066-4c7c-9d34-4c44fedf82df\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:dce0c12b-bb0f-46f4-87bf-55bba534ec14:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n",
      "\n",
      "~~~ Dispatch task ~~~\n",
      "flow_endpoint: Chat Flow\n",
      "flow_id: a54b486b-8066-4c7c-9d34-4c44fedf82df\n",
      "owner_id: local\n",
      "message_paths: ['push_tasks:716f9116-c0e4-4ce5-a5bd-d6512cf4208a:msg']\n",
      "parallel_dispatch: False\n",
      "\n",
      "Input message source: Proxy_Chat Flow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Reply~~~~ \n",
      " {'api_output': 'The capital of Switzerland is Bern.'}\n",
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'query': 'Where is it located?'}\n",
      "~~~~Reply~~~~ \n",
      " {'api_output': 'Bern is located in the central part of Switzerland, on the Swiss Plateau.'}\n"
     ]
    }
   ],
   "source": [
    "input_message1 = FlowMessage(\n",
    "    data={\"id\": 0, \"question\": \"What is the capital of Switzerland?\"},\n",
    ")\n",
    "\n",
    "input_message2 = FlowMessage(\n",
    "    data={\"id\": 0, \"query\": \"Where is it located?\"},\n",
    ")\n",
    "\n",
    "messages = [input_message1, input_message2]\n",
    "\n",
    "for msg in messages:\n",
    "    print(\"~~~~Sent message~~~~\\n\", msg.data)\n",
    "    future = english_chatbot.get_reply_future(msg)\n",
    "    reply = future.get_data()\n",
    "    print(\"~~~~Reply~~~~ \\n\",reply)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Chatting with the French ChatFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'question': 'What is the capital of Switzerland?'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Reply~~~~ \n",
      " {'api_output': 'La capitale de la Suisse est Berne.'}\n",
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'query': 'Where is it located?'}\n",
      "~~~~Reply~~~~ \n",
      " {'api_output': 'Berne est situÃ©e dans le centre de la Suisse, sur les rives de la riviÃ¨re Aar.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for msg in messages:\n",
    "    print(\"~~~~Sent message~~~~\\n\", msg.data)\n",
    "    future = french_chatbot.get_reply_future(msg)\n",
    "    reply = future.get_data()\n",
    "    print(\"~~~~Reply~~~~ \\n\",reply)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Change Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted ce9fc95d-6c90-46b1-bea2-f3275de2cb16 at flows:Chat Flow:mounts:local:ce9fc95d-6c90-46b1-bea2-f3275de2cb16\n"
     ]
    }
   ],
   "source": [
    "english_chatbot_gpt4_overrides = copy.deepcopy(cfg)\n",
    "\n",
    "english_chatbot_gpt4_overrides[\"backend\"][\"model_name\"] = {\"openai\": \"gpt-4\"}\n",
    "\n",
    "api_info = [ApiInfo(backend_used=\"openai\", api_key=os.getenv(\"OPENAI_API_KEY\"))]\n",
    "quick_load_api_keys(english_chatbot_gpt4_overrides, api_info)\n",
    "\n",
    "\n",
    "\n",
    "english_chatbot_gpt4 = serve_utils.get_flow_instance(\n",
    "    cl=cl,\n",
    "    flow_endpoint=\"Chat Flow\",\n",
    "    config_overrides=english_chatbot_gpt4_overrides,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'question': 'What is the capital of Switzerland?'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Reply~~~~ \n",
      " {'api_output': 'The capital of Switzerland is Bern.'}\n",
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'query': 'Where is it located?'}\n",
      "~~~~Reply~~~~ \n",
      " {'api_output': \"Bern is located in the west-central part of Switzerland. It's situated on a peninsula formed by the meandering waters of the River Aare.\"}\n"
     ]
    }
   ],
   "source": [
    "input_message1 = FlowMessage(\n",
    "    data={\"id\": 0, \"question\": \"What is the capital of Switzerland?\"},\n",
    ")\n",
    "\n",
    "input_message2 = FlowMessage(\n",
    "    data={\"id\": 0, \"query\": \"Where is it located?\"},\n",
    ")\n",
    "\n",
    "messages = [input_message1, input_message2]\n",
    "\n",
    "for msg in messages:\n",
    "    print(\"~~~~Sent message~~~~\\n\", msg.data)\n",
    "    future = english_chatbot_gpt4.get_reply_future(msg)\n",
    "    reply = future.get_data()\n",
    "    print(\"~~~~Reply~~~~ \\n\",reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Do it yourself: Mount a ChatFlow and personalize it (Actions required)\n",
    "\n",
    "\n",
    "- Option 1: Mount a Chatflow who generates text with a `temperature = 1` and always ends replies with \"my good friend\"\n",
    "    - Hint: To format the reply, play around with `system_message_prompt_template`\n",
    "\n",
    "- Option 2: Personalize your flow as you like !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"_target_\": \"flow_modules.aiflows.ChatFlowModule.ChatAtomicFlow.instantiate_from_default_config\",\n",
      "    \"name\": \"SimpleQA_Flow\",\n",
      "    \"description\": \"A flow that answers questions.\",\n",
      "    \"input_interface_non_initialized\": [\n",
      "        \"question\"\n",
      "    ],\n",
      "    \"backend\": {\n",
      "        \"_target_\": \"aiflows.backends.llm_lite.LiteLLMBackend\",\n",
      "        \"api_infos\": \"???\",\n",
      "        \"model_name\": {\n",
      "            \"openai\": \"gpt-3.5-turbo\",\n",
      "            \"azure\": \"azure/gpt-4\"\n",
      "        },\n",
      "        \"n\": 1,\n",
      "        \"max_tokens\": 3000,\n",
      "        \"temperature\": 0.3,\n",
      "        \"top_p\": 0.2,\n",
      "        \"frequency_penalty\": 0,\n",
      "        \"presence_penalty\": 0\n",
      "    },\n",
      "    \"n_api_retries\": 6,\n",
      "    \"wait_time_between_retries\": 20,\n",
      "    \"system_message_prompt_template\": {\n",
      "        \"_target_\": \"aiflows.prompt_template.JinjaPrompt\",\n",
      "        \"template\": \"You are a helpful chatbot that truthfully answers questions.\",\n",
      "        \"input_variables\": [],\n",
      "        \"partial_variables\": {}\n",
      "    },\n",
      "    \"init_human_message_prompt_template\": {\n",
      "        \"_target_\": \"aiflows.prompt_template.JinjaPrompt\",\n",
      "        \"template\": \"Answer the following question: {{question}}\",\n",
      "        \"input_variables\": [\n",
      "            \"question\"\n",
      "        ],\n",
      "        \"partial_variables\": {}\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# This is what the demo config looks like ! Modify it to your needs\n",
    "print(json.dumps(cfg, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted a54b486b-8066-4c7c-9d34-4c44fedf82df at flows:Chat Flow:mounts:local:a54b486b-8066-4c7c-9d34-4c44fedf82df\n"
     ]
    }
   ],
   "source": [
    "# Deepcopy of demo config\n",
    "overrides_config = copy.deepcopy(cfg)\n",
    "\n",
    "#TODO: Mount a Chatflow who generates text with a `temperature = 1` and a `system_message_prompt_template` that is personalized\n",
    "overrides_config[\"backend\"][\"temperature\"] = 1.0\n",
    "overrides_config[\"system_message_prompt_template\"][\"template\"] = \\\n",
    "    \"You are a helpful chatbot that fills out a given prompt in areas surrounded by brackets '[[]]'. Within the brackets are instructions you should follow.\"\n",
    "\n",
    "\n",
    "#loading api key to config\n",
    "api_info = [ApiInfo(backend_used=\"openai\", api_key=os.getenv(\"OPENAI_API_KEY\"))]\n",
    "quick_load_api_keys(overrides_config, api_info)\n",
    "\n",
    "\n",
    "#TO DO get a flow instance\n",
    "personalized_chatbot = serve_utils.get_flow_instance(\n",
    "    cl=cl,\n",
    "    flow_endpoint=\"Chat Flow\",\n",
    "    config_overrides=overrides_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'question': 'The capital of Switzerland is [[insert capital]].'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Reply~~~~ \n",
      " The capital of Switzerland is Bern.\n",
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'query': \"[[Write back Larry as an email and thank him for the pertinant questions ]]. The translation of 'Ciao come stai ?'             is: [[insert translation and explain the meaning of each word seperately]]. [[Sign of with my name: Nicolas Baldwin]]\"}\n",
      "~~~~Reply~~~~ \n",
      " Subject: Thank You for Your Pertinent Questions\n",
      "\n",
      "Hi Larry,\n",
      "\n",
      "I wanted to express my gratitude for your pertinent questions. Your engagement is truly appreciated.\n",
      "\n",
      "The translation of 'Ciao come stai?' is as follows:\n",
      "\n",
      "- 'Ciao' means 'hello' or 'hi'.\n",
      "- 'Come' means 'how'.\n",
      "- 'Stai' is the informal second-person singular form of the verb 'stare', which means 'are you'.\n",
      "\n",
      "So, 'Ciao come stai?' translates to 'Hello, how are you?' in English.\n",
      "\n",
      "Thank you once again for your questions.\n",
      "\n",
      "Best regards,\n",
      "Nicolas Baldwin\n"
     ]
    }
   ],
   "source": [
    "input_message1 = FlowMessage(\n",
    "    data={\"id\": 0, \"question\": \"The capital of Switzerland is [[insert capital]].\"},\n",
    ")\n",
    "\n",
    "input_message2 = FlowMessage(\n",
    "    data={\n",
    "        \"id\": 0,\n",
    "        \"query\": \"[[Write back Margaret as an email and thank her for the pertinant questions ]]. The translation of 'Ciao come stai ?' \\\n",
    "            is: [[insert translation and explain the meaning of each word seperately]]. [[Sign of with my name: Nicolas Baldwin]]\"\n",
    "    },\n",
    ")\n",
    "\n",
    "messages = [input_message1, input_message2]\n",
    "\n",
    "for msg in messages:\n",
    "    print(\"~~~~Sent message~~~~\\n\", msg.data)\n",
    "    future = personalized_chatbot.get_reply_future(msg)\n",
    "    reply = future.get_data()\n",
    "    print(\"~~~~Reply~~~~ \\n\",reply[\"api_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coflows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
