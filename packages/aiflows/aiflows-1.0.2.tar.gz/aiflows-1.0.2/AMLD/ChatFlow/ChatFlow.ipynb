{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2\n",
    "\n",
    "**In this tutorial you will:**\n",
    "- learn how to pull a flow from the FlowVerse [Section 1](#1-flowverse)\n",
    "- Familiarized yourself with the ChatAtomicFlow [Section 2](#2-chatatomicflow)\n",
    "- Learn how to customize existing flows to your needs [Section 2](#231-customizing-chatatomicflow-creating-a-chatbot-that-answers-in-a-given-language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolasbaldwin/opt/miniconda3/envs/mockenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#imports\n",
    "from aiflows.utils.general_helpers import read_yaml_file, quick_load_api_keys\n",
    "from aiflows.utils import serve_utils\n",
    "from aiflows.utils import colink_utils\n",
    "from aiflows.workers import run_dispatch_worker_thread\n",
    "from aiflows.base_flows import AtomicFlow\n",
    "from aiflows.messages import FlowMessage\n",
    "from aiflows import flow_verse\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "from utils import compile_and_writefile, dict_to_yaml\n",
    "import json\n",
    "import copy\n",
    "#Specify path of your flow modules\n",
    "FLOW_MODULES_PATH = \"./\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting a local colink server\n",
    "cl = colink_utils.start_colink_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2024-03-22 07:59:09,214\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:220\u001b[0m][\u001b[32mINFO\u001b[0m] - Dispatch worker started in attached thread.\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:09,224\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:221\u001b[0m][\u001b[32mINFO\u001b[0m] - dispatch_point: coflows_dispatch\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Â Start Worker thread\n",
    "run_dispatch_worker_thread(cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FlowVerse\n",
    "\n",
    "### 1.1 What's the FlowVerse ? \n",
    "The FlowVerse is the hub of flows created and shared by our amazing community for everyone to use! These flows are shared on Hugging Face with the intention of being reused by others. Explore our Flows on the FlowVerse [here](https://huggingface.co/aiflows)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Pulling a Flow from the FlowVerse\n",
    "\n",
    "To pull the `ChatFlowModule` (check out its card [here](https://huggingface.co/aiflows/ChatFlowModule)) from the FlowVerse, we need to use the `flow_verse.sync_dependencies` function. This function will pull the flow into the current directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2024-03-22 07:59:10,790\u001b[0m][\u001b[34maiflows.flow_verse.loading:775\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m[<interactive>]\u001b[0m started to sync flow module dependencies to /Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/AMLD/ChatFlow/flow_modules...\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:11,135\u001b[0m][\u001b[34maiflows.flow_verse.loading:608\u001b[0m][\u001b[32mINFO\u001b[0m] - aiflows/ChatFlowModule:coflows already synced, skip\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:11,138\u001b[0m][\u001b[34maiflows.flow_verse.loading:825\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[32m[<interactive>]\u001b[0m finished syncing\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/nicolasbaldwin/Documents/OneDrive/EPFL/DLAB/aiflow-colink/aiflows/AMLD/ChatFlow/flow_modules/aiflows/ChatFlowModule']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from aiflows import flow_verse\n",
    "# ~~~ Load Flow dependecies from FlowVerse ~~~\n",
    "dependencies = [\n",
    "    {\"url\": \"aiflows/ChatFlowModule\", \"revision\": \"coflows\"},\n",
    "]\n",
    "\n",
    "flow_verse.sync_dependencies(dependencies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this down:\n",
    "* `dependencies` is a list of dictionaries (in this case, there's only one) indicating which FlowModules we want to pull from the FlowVerse. The dictionary contains two key-value pairs:\n",
    "  * `url`: Specifies the URL where the flow can be found on Hugging Face. Here, the URL is `aiflows/ChatFlowModule`, where `aiflows` is the name of our organization on Hugging Face (or the username of a user hosting their flow on Hugging Face), and `ChatFlowModule` is the name of the FlowModule containing the `ChatAtomicFlow` on the FlowVerse. Note that the `url` is literally the address of the `ChatFlowModule` on Hugging Face (excluding the https://huggingface.co/). So if you type https://huggingface.co/aiflows/ChatFlowModule in your browser, you will find the Flow.\n",
    "  * `revision`: Represents the revision id (i.e., the full commit hash) of the commit we want to fetch. Note that if you set `revision` to `main`, it will fetch the latest commit on the main branch.\n",
    "\n",
    "Note that running the cell above will pull the flow in **flow_modules/aiflows/ChatFlowModule**.\n",
    "\n",
    "Now that we've fetched the `ChatAtomicFlowModule` from the FlowVerse, we can start creating our own personalized Flow from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ChatAtomicFlow\n",
    "\n",
    "The ChatAtomicFlow is a minimal wrapper for querying an LLM via an API to generate textuals responses to textual inputs. It employs litellm as a backend to query the LLM via an API. See litellm's supported models and APIs here: https://docs.litellm.ai/docs/providers. In this tutorial, we will be using `openai` as the provider. But you can use any provider supported by litellm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Inspecting ChatAtomicFlow's Default Configuration\n",
    "\n",
    "Let's start by inspecting the default configuration of the `ChatAtomicFlow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"_target_\": \"flow_modules.aiflows.ChatFlowModule.ChatAtomicFlow.instantiate_from_default_config\",\n",
      "    \"name\": \"ChatAtomicFlow\",\n",
      "    \"description\": \"Flow which uses as tool an LLM though an API\",\n",
      "    \"enable_cache\": true,\n",
      "    \"n_api_retries\": 6,\n",
      "    \"wait_time_between_retries\": 20,\n",
      "    \"system_name\": \"system\",\n",
      "    \"user_name\": \"user\",\n",
      "    \"assistant_name\": \"assistant\",\n",
      "    \"backend\": {\n",
      "        \"_target_\": \"aiflows.backends.llm_lite.LiteLLMBackend\",\n",
      "        \"api_infos\": \"???\",\n",
      "        \"model_name\": {\n",
      "            \"openai\": \"gpt-3.5-turbo\"\n",
      "        },\n",
      "        \"n\": 1,\n",
      "        \"max_tokens\": 2000,\n",
      "        \"temperature\": 0.3,\n",
      "        \"top_p\": 0.2,\n",
      "        \"frequency_penalty\": 0,\n",
      "        \"presence_penalty\": 0,\n",
      "        \"stream\": true\n",
      "    },\n",
      "    \"system_message_prompt_template\": {\n",
      "        \"_target_\": \"aiflows.prompt_template.JinjaPrompt\"\n",
      "    },\n",
      "    \"init_human_message_prompt_template\": {\n",
      "        \"_target_\": \"aiflows.prompt_template.JinjaPrompt\",\n",
      "        \"template\": \"Answer the following question: {{query}}\",\n",
      "        \"input_variables\": [\n",
      "            \"query\"\n",
      "        ],\n",
      "        \"partial_variables\": {}\n",
      "    },\n",
      "    \"human_message_prompt_template\": {\n",
      "        \"_target_\": \"aiflows.prompt_template.JinjaPrompt\",\n",
      "        \"template\": \"{{query}}\",\n",
      "        \"input_variables\": [\n",
      "            \"query\"\n",
      "        ]\n",
      "    },\n",
      "    \"input_interface_initialized\": [\n",
      "        \"query\"\n",
      "    ],\n",
      "    \"input_interface_non_initialized\": [\n",
      "        \"query\"\n",
      "    ],\n",
      "    \"query_message_prompt_template\": {\n",
      "        \"_target_\": \"aiflows.prompt_template.JinjaPrompt\"\n",
      "    },\n",
      "    \"previous_messages\": {\n",
      "        \"first_k\": null,\n",
      "        \"last_k\": null\n",
      "    },\n",
      "    \"output_interface\": [\n",
      "        \"api_output\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Print the default configuration\n",
    "default_cfg = read_yaml_file(\"flow_modules/aiflows/ChatFlowModule/ChatAtomicFlow.yaml\")\n",
    "print(json.dumps(default_cfg, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full list of parameters and their descriptions, please refer to the flow's card [here](https://huggingface.co/aiflows/ChatFlowModule#chatatomicflow-objects) (under configuration parameters). We will be discussing the most important parameters in this tutorial:\n",
    "\n",
    "- **`input_interface_initialized` and `input_interface_non_initialized`**: These parameters in our configuration specify the keys expected in the input data dictionary when the `ChatAtomicFlow` is called for the first time and subsequently. \n",
    "  - `input_interface_non_initialized`: Specifies the keys expected in the input data dictionary when the `ChatAtomicFlow` is called for the first time the flow is called.\n",
    "  - `input_interface_initialized`: Specifies the keys expected in the input data dictionary after the first call, essentially serving a role similar to the regular `input_interface`. \n",
    "The distinction between the two becomes apparent when different inputs are required for the initial query compared to subsequent queries. \n",
    "For instance, in ReAct, the first time you query the LLM, the input is provided by a human, such as a question. In subsequent queries, the input comes from the execution of a tool (e.g., a query to Wikipedia). In ReAct's case, these two scenarios are distinguished by `ChatAtomicFlow`'s `input_interface_non_initialized` and `input_interface_initialized` parameters.\n",
    "\n",
    "-  `backend` is a dictionary containing parameters specific to the LLM. These parameters include:\n",
    "    - `api_infos`: Your API information (which will be passed later for privacy reasons).\n",
    "    - `model_name`: A dictionary with key-item pairs, where keys correspond to the `backend_used` attribute of the `ApiInfo` class for the chosen backend, and values represent the desired model for that backend. Model selection depends on the provided `api_infos`. Additional models can be added for different backends, following LiteLLM's naming conventions (refer to LiteLLM's supported providers and model names [here](https://docs.litellm.ai/docs/providers)). For instance, with an Anthropic API key and a desire to use \"claude-2,\" one would check Anthropic's model details [here](https://docs.litellm.ai/docs/providers/anthropic#model-details). As \"claude-2\" is named the same in LiteLLM, the `model_name` dictionary would be updated as follows:\n",
    "      ```yaml\n",
    "      backend:\n",
    "      _target_: aiflows.backends.llm_lite.LiteLLMBackend\n",
    "      api_infos: ???\n",
    "      model_name:\n",
    "        openai: \"gpt-3.5-turbo\"\n",
    "        anthropic: \"claude-2\"\n",
    "      ```\n",
    "    - `n`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty` are generation parameters for LiteLLM's completion function (refer to all possible generation parameters [here](https://docs.litellm.ai/docs/completion/input#input-params-1)).\n",
    "\n",
    "- `system_message_prompt_template`: This is the system prompt template passed to the LLM.\n",
    "- `init_human_message_prompt_template`: This is the user prompt template passed to the LLM the first time the flow is called.\n",
    "- `human_message_prompt_template`: This is the user prompt template passed to the LLM after the first time the flow is called.\n",
    "\n",
    "All 3 prompts are in Jinja format and contain the following configurable parameter:\n",
    "  - `template`: The prompt template in Jinja format.\n",
    "  - `input_variables`: The input variables of the prompt to be passed to the Jinja template (passed when the flow is called).\n",
    "  - `partial_variables`: The partial variables of the prompt to be passed to the Jinja template (passed directly from the config).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Serving the default Configuration\n",
    "\n",
    "Now that we have a basic understanding of the `ChatAtomicFlow`'s configuration, let's serve the default configuration of the `ChatAtomicFlow` by calling the `serve` method. This method will return the default configuration of the flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2024-03-22 07:59:11,585\u001b[0m][\u001b[34maiflows.utils.serve_utils:116\u001b[0m][\u001b[32mINFO\u001b[0m] - Started serving flow_modules.aiflows.ChatFlowModule.ChatAtomicFlow at flows:ChatAtomicFlow.\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:11,586\u001b[0m][\u001b[34maiflows.utils.serve_utils:117\u001b[0m][\u001b[32mINFO\u001b[0m] - dispatch_point: coflows_dispatch\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:11,593\u001b[0m][\u001b[34maiflows.utils.serve_utils:118\u001b[0m][\u001b[32mINFO\u001b[0m] - parallel_dispatch: False\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:11,595\u001b[0m][\u001b[34maiflows.utils.serve_utils:119\u001b[0m][\u001b[32mINFO\u001b[0m] - singleton: False\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve_utils.serve_flow(\n",
    "    cl=cl,\n",
    "    flow_class_name=\"flow_modules.aiflows.ChatFlowModule.ChatAtomicFlow\",\n",
    "    flow_endpoint=\"ChatAtomicFlow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Customizing ChatAtomicFlow: Creating a Chatbot that Answers in a given Language\n",
    "\n",
    "To showcase how to customize the `ChatAtomicFlow`, we will create a chatbot that answers in a given language. We will use the `ChatAtomicFlow` to create a chatbot that answers in French. The key here is to understand how to get an instance of the `ChatAtomicFlow` with your desired configuration overriding the default configuration.\n",
    "\n",
    "To do this, we will:\n",
    "- 1. Copy the default configuration of the `ChatAtomicFlow`\n",
    "- 2. Override the `system_message_prompt_template` to include the language we want the answer in\n",
    "- 3. Load our API key in the `api_infos` field of the configuration\n",
    "- 4. Get an instance of the `ChatAtomicFlow` with our desired configuration overrides\n",
    "- 5. Run the `ChatAtomicFlow` with our desired configuration !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2024-03-22 07:59:11,900\u001b[0m][\u001b[34maiflows.utils.serve_utils:336\u001b[0m][\u001b[32mINFO\u001b[0m] - Mounted 4e62516b-73f7-43a7-be7b-30af64588fea at flows:ChatAtomicFlow:mounts:local:4e62516b-73f7-43a7-be7b-30af64588fea\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Copy the default configuration of the `ChatAtomicFlow`\n",
    "\n",
    "chatbot_overrides = copy.deepcopy(default_cfg)\n",
    "\n",
    "# STEP 2: Override the system_message_prompt_template to prompt the user to speak in French\n",
    "language = \"French\"\n",
    "chatbot_overrides[\"system_message_prompt_template\"][\"template\"] = \\\n",
    "f'You are a helpful chatbot that truthfully answers questions. Answer in the following language: {language}.'\n",
    "\n",
    "# STEP 3: Load the API keys\n",
    "from aiflows.backends.api_info import ApiInfo\n",
    "\n",
    "api_info = [ApiInfo(backend_used=\"openai\", api_key=os.getenv(\"OPENAI_API_KEY\"))]\n",
    "quick_load_api_keys(chatbot_overrides, api_info)\n",
    "\n",
    "#STEP 4: Get an instance of the `ChatAtomicFlow` with the overrides\n",
    "french_chatbot = serve_utils.get_flow_instance(\n",
    "    cl=cl,\n",
    "    flow_endpoint=\"ChatAtomicFlow\",\n",
    "    config_overrides=chatbot_overrides,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'query': 'What is the capital of Switzerland?'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2024-03-22 07:59:12,133\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:113\u001b[0m][\u001b[32mINFO\u001b[0m] - \n",
      "~~~ Dispatch task ~~~\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:12,140\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:155\u001b[0m][\u001b[32mINFO\u001b[0m] - flow_endpoint: ChatAtomicFlow\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:12,144\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:156\u001b[0m][\u001b[32mINFO\u001b[0m] - flow_id: 4e62516b-73f7-43a7-be7b-30af64588fea\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:12,149\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:157\u001b[0m][\u001b[32mINFO\u001b[0m] - owner_id: local\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:12,151\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:158\u001b[0m][\u001b[32mINFO\u001b[0m] - message_paths: ['push_tasks:8aac5bf0-4972-4361-8e7c-93beb81a88f5:msg']\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:12,155\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:159\u001b[0m][\u001b[32mINFO\u001b[0m] - parallel_dispatch: False\n",
      "\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:12,255\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:182\u001b[0m][\u001b[32mINFO\u001b[0m] - Input message source: Proxy_ChatAtomicFlow\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:13,327\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:113\u001b[0m][\u001b[32mINFO\u001b[0m] - \n",
      "~~~ Dispatch task ~~~\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:13,354\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:155\u001b[0m][\u001b[32mINFO\u001b[0m] - flow_endpoint: ChatAtomicFlow\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:13,356\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:156\u001b[0m][\u001b[32mINFO\u001b[0m] - flow_id: 4e62516b-73f7-43a7-be7b-30af64588fea\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:13,359\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:157\u001b[0m][\u001b[32mINFO\u001b[0m] - owner_id: local\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:13,380\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:158\u001b[0m][\u001b[32mINFO\u001b[0m] - message_paths: ['push_tasks:c19c39fa-afca-4118-9db9-97418ee0edc7:msg']\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:13,393\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:159\u001b[0m][\u001b[32mINFO\u001b[0m] - parallel_dispatch: False\n",
      "\u001b[0m\n",
      "[\u001b[36m2024-03-22 07:59:13,615\u001b[0m][\u001b[34maiflows.workers.dispatch_worker:182\u001b[0m][\u001b[32mINFO\u001b[0m] - Input message source: Proxy_ChatAtomicFlow\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Reply~~~~ \n",
      " {'api_output': 'La capitale de la Suisse est Berne.'}\n",
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'query': 'Where is it located?'}\n",
      "~~~~Reply~~~~ \n",
      " {'api_output': 'Berne est situÃ©e dans le centre de la Suisse, sur les rives de la riviÃ¨re Aar.'}\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Send messages to the chatbot\n",
    "input_message1 = FlowMessage(\n",
    "    data={\"id\": 0, \"query\": \"What is the capital of Switzerland?\"},\n",
    ")\n",
    "\n",
    "input_message2 = FlowMessage(\n",
    "    data={\"id\": 0, \"query\": \"Where is it located?\"},\n",
    ")\n",
    "\n",
    "messages = [input_message1, input_message2]\n",
    "\n",
    "for msg in messages:\n",
    "    print(\"~~~~Sent message~~~~\\n\", msg.data)\n",
    "    future = french_chatbot.get_reply_future(msg)\n",
    "    reply = future.get_data()\n",
    "    print(\"~~~~Reply~~~~ \\n\",reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Customizing ChatAtomicFlow: Changing the model\n",
    "\n",
    "Now we could also change model to use a different language model. For example, we could use the `gpt-4` model to answer in French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2024-03-22 00:59:17,573\u001b[0m][\u001b[34maiflows.utils.serve_utils:336\u001b[0m][\u001b[32mINFO\u001b[0m] - Mounted c9224c21-c2fb-44e4-91dc-487f77fd3659 at flows:ChatAtomicFlow:mounts:local:c9224c21-c2fb-44e4-91dc-487f77fd3659\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Copy the default configuration of the `ChatAtomicFlow`\n",
    "english_chatbot_gpt4_overrides = copy.deepcopy(default_cfg)\n",
    "\n",
    "# STEP 2: Override the model_name to use GPT-4\n",
    "english_chatbot_gpt4_overrides[\"backend\"][\"model_name\"] = {\"openai\": \"gpt-4\"}\n",
    "\n",
    "# STEP 3: Load the API keys\n",
    "api_info = [ApiInfo(backend_used=\"openai\", api_key=os.getenv(\"OPENAI_API_KEY\"))]\n",
    "quick_load_api_keys(english_chatbot_gpt4_overrides, api_info)\n",
    "\n",
    "#STEP 4: Get an instance of the `ChatAtomicFlow` with the overrides\n",
    "english_chatbot_gpt4 = serve_utils.get_flow_instance(\n",
    "    cl=cl,\n",
    "    flow_endpoint=\"ChatAtomicFlow\",\n",
    "    config_overrides=english_chatbot_gpt4_overrides,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'query': 'What is the capital of Switzerland?'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Reply~~~~ \n",
      " {'api_output': 'The capital of Switzerland is Bern.'}\n",
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'query': 'Where is it located?'}\n",
      "~~~~Reply~~~~ \n",
      " {'api_output': 'Bern is located in the Swiss Plateau, which is in the central part of Switzerland. It is surrounded by the Aare River.'}\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Send messages to the chatbot\n",
    "\n",
    "input_message1 = FlowMessage(\n",
    "    data={\"id\": 0, \"query\": \"What is the capital of Switzerland?\"},\n",
    ")\n",
    "\n",
    "input_message2 = FlowMessage(\n",
    "    data={\"id\": 0, \"query\": \"Where is it located?\"},\n",
    ")\n",
    "\n",
    "messages = [input_message1, input_message2]\n",
    "\n",
    "for msg in messages:\n",
    "    print(\"~~~~Sent message~~~~\\n\", msg.data)\n",
    "    future = english_chatbot_gpt4.get_reply_future(msg)\n",
    "    reply = future.get_data()\n",
    "    print(\"~~~~Reply~~~~ \\n\",reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Customizing ChatAtomicFlow: Creating an instruction-based chatbot\n",
    "\n",
    "Now let's write a chatbot that completes your messages by substituting whatever you write in double brackets \"[[]]\" with the instructions you give it. For example, \"the capital of Switzerland is [[insert capital]]\" should return \"the capital of Switzerland is Bern\". This is a simple example of a chatbot that could augment your writing by providing you with the information you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[36m2024-03-22 00:59:29,523\u001b[0m][\u001b[34maiflows.utils.serve_utils:336\u001b[0m][\u001b[32mINFO\u001b[0m] - Mounted a01623d5-1efb-4887-ba65-9e9d06dcc6d7 at flows:ChatAtomicFlow:mounts:local:a01623d5-1efb-4887-ba65-9e9d06dcc6d7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Deepcopy of demo config\n",
    "overrides_config = copy.deepcopy(default_cfg)\n",
    "\n",
    "#TODO: Mount a Chatflow who generates text with a `temperature = 1` and a `system_message_prompt_template` that is personalized\n",
    "overrides_config[\"backend\"][\"temperature\"] = 1.0\n",
    "overrides_config[\"system_message_prompt_template\"][\"template\"] = \\\n",
    "    \"You are a helpful chatbot that fills out a given prompt in areas surrounded by brackets '[[]]'. Within the brackets are instructions you should follow.\"\n",
    "\n",
    "\n",
    "#loading api key to config\n",
    "api_info = [ApiInfo(backend_used=\"openai\", api_key=os.getenv(\"OPENAI_API_KEY\"))]\n",
    "quick_load_api_keys(overrides_config, api_info)\n",
    "\n",
    "\n",
    "#TO DO get a flow instance\n",
    "personalized_chatbot = serve_utils.get_flow_instance(\n",
    "    cl=cl,\n",
    "    flow_endpoint=\"ChatAtomicFlow\",\n",
    "    config_overrides=overrides_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'query': 'The capital of Switzerland is [[insert capital]].'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~Reply~~~~ \n",
      " The capital of Switzerland is [[Bern]].\n",
      "~~~~Sent message~~~~\n",
      " {'id': 0, 'query': \"\\nDear Margaret,\\n[[Ask about her day and talk about the sunny weather]]\\n\\n[[Ask if she has finished her part of the report]]. I've finished my part and I'm looking forward to the weekend!\\n\\n[[Sign of with my name: Nicolas Baldwin]].\\n\"}\n",
      "~~~~Reply~~~~ \n",
      " Dear Margaret,\n",
      "\n",
      "How has your day been? The weather is so sunny and beautiful today!\n",
      "\n",
      "Have you finished your part of the report yet? I've completed my section and I'm excited for the weekend!\n",
      "\n",
      "Take care,\n",
      "Nicolas Baldwin\n"
     ]
    }
   ],
   "source": [
    "input_message1 = FlowMessage(\n",
    "    data={\"id\": 0, \"query\": \"The capital of Switzerland is [[insert capital]].\"},\n",
    ")\n",
    "\n",
    "query_2 = \\\n",
    "\"\"\"\n",
    "Dear Margaret,\n",
    "[[Ask about her day and talk about the sunny weather]]\n",
    "\n",
    "[[Ask if she has finished her part of the report]]. I've finished my part and I'm looking forward to the weekend!\n",
    "\n",
    "[[Sign of with my name: Nicolas Baldwin]].\n",
    "\"\"\"\n",
    "\n",
    "input_message2 = FlowMessage(\n",
    "    data={\n",
    "        \"id\": 0,\n",
    "        \"query\": query_2\n",
    "    },\n",
    ")\n",
    "\n",
    "messages = [input_message1, input_message2]\n",
    "\n",
    "for msg in messages:\n",
    "    print(\"~~~~Sent message~~~~\\n\", msg.data)\n",
    "    future = personalized_chatbot.get_reply_future(msg)\n",
    "    reply = future.get_data()\n",
    "    print(\"~~~~Reply~~~~ \\n\",reply[\"api_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coflows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
