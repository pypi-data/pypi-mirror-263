{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/22 10:45:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by local backend from path: obj_det/chkpts/mask_reppointsv2_swin_tiny_patch4_window7_3x.pth\n",
      "Done!!, save to obj_det/chkpts/mask_reppointsv2_swin_tiny_patch4_window7_3x_converted-dd7afe98.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:39: UserWarning: ``build_assigner`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_assigner`` would be deprecated soon, please use '\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:24: UserWarning: ``build_iou_calculator`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn(\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:46: UserWarning: ``build_sampler`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_sampler`` would be deprecated soon, please use '\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/apis/inference.py:90: UserWarning: dataset_meta or class names are not saved in the checkpoint's meta data, use COCO classes by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: obj_det/chkpts/mask_reppointsv2_swin_tiny_patch4_window7_3x_converted-dd7afe98.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:39: UserWarning: ``build_assigner`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_assigner`` would be deprecated soon, please use '\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:24: UserWarning: ``build_iou_calculator`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn(\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:46: UserWarning: ``build_sampler`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_sampler`` would be deprecated soon, please use '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DetDataPreprocessor()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !mim download mmdet --config mask-rcnn_swin-t-p4-w7_fpn_1x_coco --dest .\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmengine.config import Config\n",
    "from cfm_task_models.legacy import *\n",
    "\n",
    "from cfm_task_models.split_utils import SplitSwinTransformer, SplitTwoStageDetector, TwoInputIdentity, SplitRepPointsV2MaskDetector\n",
    "\n",
    "config_file_old = 'obj_det/cfgs/mask-rcnn_swin-t-p4-w7_fpn_1x_coco.py'\n",
    "checkpoint_file_old = 'obj_det/chkpts/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth'\n",
    "config_file = 'obj_det/cfgs/mask_reppointsv2_swin_t_modified.py'\n",
    "checkpoint_file = 'obj_det/chkpts/mask_reppointsv2_swin_tiny_patch4_window7_3x.pth'\n",
    "\n",
    "\n",
    "# model_init = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n",
    "# result = inference_detector(model, 'demo/demo.jpg')\n",
    "\n",
    "# from mmdet.registry import MODELS\n",
    "# MODELS.get('SwinTransformer')\n",
    "# MODELS.get('TwoStageDetector')\n",
    "model = SplitRepPointsV2MaskDetector.create_from_cfg_and_checkpoint(config_file, checkpoint_file)\n",
    "model.frontend_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_file: /local-scratch2/aharell/CFM-Task-Models/data/coco/annotations/coco2017_val_split_anns.json\n",
      "ann_file: /local-scratch2/aharell/CFM-Task-Models/data/coco/annotations/coco2017_val_split_anns.json\n",
      "backend_args: None\n",
      "backend_args: None\n",
      "format_only: False\n",
      "format_only: False\n",
      "metric: ['bbox', 'segm']\n",
      "metric: ['bbox', 'segm']\n",
      "type: CocoMetric\n",
      "type: CocoMetric\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "cfg_old = Config.fromfile('/local-scratch2/aharell/Cheng_Yolo_Min_Repo/examples/newer_models/cfgs/swin_tiny_mask_rcnn_simplified_cfg.py')\n",
    "# model = SplitTwoStageDetector.create_from_instance_and_cfg(model_init, cfg, cut_point=1)\n",
    "\n",
    "# prp = deepcopy(model.data_preprocessor)\n",
    "# model.data_preprocessor = TwoInputIdentity()\n",
    "\n",
    "# print(model.frontend_preprocessor)\n",
    "# print(cfg_old['train_cfg'])\n",
    "# print(cfg_old['optim_wrapper'])\n",
    "cfg = model.cfg\n",
    "# cfg.update({'train_cfg':cfg_old['train_cfg'], 'optim_wrapper': })\n",
    "# print(model.cfg.model['train_cfg'])\n",
    "# print(model.cfg['optimizer'])\n",
    "# print(cfg['train_dataloader'])\n",
    "# print(cfg_old['train_dataloader'])\n",
    "for k in cfg['val_evaluator']:\n",
    "    print(f'{k}: {cfg[\"val_evaluator\"][k]}')\n",
    "    print(f'{k}: {cfg_old[\"val_evaluator\"][k]}' if k in cfg_old['val_evaluator'] else f'{k} is not in the old config')\n",
    "# model.backbone = SplitSwinTransformer.create_from_instance_and_cfg(model.backbone, cfg)\n",
    "\n",
    "cfg['val_evaluator'] = cfg_old['val_evaluator']\n",
    "cfg['test_evaluator'] = cfg_old['test_evaluator']\n",
    "cfg.dump('mask_reppointsv2_swin_t_modified.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: obj_det/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TwoInputIdentity()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = SplitTwoStageDetector.create_from_cfg_and_checkpoint(config_file, checkpoint_file)\n",
    "model2.frontend_preprocessor\n",
    "model2.data_preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "train_dataloader, train_cfg, and optim_wrapper should be either all None or not None, but got train_dataloader={'batch_sampler': {'type': 'AspectRatioBatchSampler'}, 'batch_size': 2, 'dataset': {'ann_file': 'annotations/instances_train2017.json', 'backend_args': None, 'data_prefix': {'img': 'train2017/'}, 'data_root': '../data/coco/', 'filter_cfg': {'filter_empty_gt': True, 'min_size': 32}, 'pipeline': [{'backend_args': None, 'type': 'LoadImageFromFile'}, {'type': 'LoadAnnotations', 'with_bbox': True, 'with_mask': True}, {'keep_ratio': True, 'scale': (1333, 800), 'type': 'Resize'}, {'prob': 0.5, 'type': 'RandomFlip'}, {'type': 'PackDetInputs'}], 'type': 'CocoDataset'}, 'num_workers': 2, 'persistent_workers': True, 'sampler': {'shuffle': True, 'type': 'DefaultSampler'}}, train_cfg=None, optim_wrapper=None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m cfg_tr[\u001b[39m'\u001b[39m\u001b[39mval_evaluator\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mann_file\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../data/coco/annotations/instances_val2017.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[39m# print(list(cfg_tr.train_dataloader.keys()))\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m runner \u001b[39m=\u001b[39m RUNNERS\u001b[39m.\u001b[39;49mbuild(cfg_tr)\n\u001b[1;32m     22\u001b[0m \u001b[39m# print(cfg_tr.train_dataloader)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# with open('data/coco/annotations/instances_train2017.json', 'r') as file:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m#     data_json = json.load(file)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# print(train_pipeline)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# print(list(data_json.keys()))\u001b[39;00m\n",
      "File \u001b[0;32m/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmengine/registry/registry.py:570\u001b[0m, in \u001b[0;36mRegistry.build\u001b[0;34m(self, cfg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m, cfg: \u001b[39mdict\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    549\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build an instance.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \n\u001b[1;32m    551\u001b[0m \u001b[39m    Build an instance by calling :attr:`build_func`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39m        >>> model = MODELS.build(cfg)\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_func(cfg, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, registry\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmengine/registry/build_functions.py:196\u001b[0m, in \u001b[0;36mbuild_runner_from_cfg\u001b[0;34m(cfg, registry)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtype must be a str or valid type, but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(obj_type)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m runner \u001b[39m=\u001b[39m runner_cls\u001b[39m.\u001b[39;49mfrom_cfg(args)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    197\u001b[0m print_log(\n\u001b[1;32m    198\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAn `\u001b[39m\u001b[39m{\u001b[39;00mrunner_cls\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` instance is built from \u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# type: ignore # noqa: E501\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mregistry, its implementation can be found in\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    200\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrunner_cls\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     logger\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcurrent\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    202\u001b[0m     level\u001b[39m=\u001b[39mlogging\u001b[39m.\u001b[39mDEBUG)\n\u001b[1;32m    203\u001b[0m \u001b[39mreturn\u001b[39;00m runner\n",
      "File \u001b[0;32m/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmengine/runner/runner.py:462\u001b[0m, in \u001b[0;36mRunner.from_cfg\u001b[0;34m(cls, cfg)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Build a runner from config.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \n\u001b[1;32m    454\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39m    Runner: A runner build from ``cfg``.\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    461\u001b[0m cfg \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(cfg)\n\u001b[0;32m--> 462\u001b[0m runner \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    463\u001b[0m     model\u001b[39m=\u001b[39;49mcfg[\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    464\u001b[0m     work_dir\u001b[39m=\u001b[39;49mcfg[\u001b[39m'\u001b[39;49m\u001b[39mwork_dir\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    465\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mtrain_dataloader\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    466\u001b[0m     val_dataloader\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mval_dataloader\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    467\u001b[0m     test_dataloader\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mtest_dataloader\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    468\u001b[0m     train_cfg\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mtrain_cfg\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    469\u001b[0m     val_cfg\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mval_cfg\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    470\u001b[0m     test_cfg\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mtest_cfg\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    471\u001b[0m     auto_scale_lr\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mauto_scale_lr\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    472\u001b[0m     optim_wrapper\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39moptim_wrapper\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    473\u001b[0m     param_scheduler\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mparam_scheduler\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    474\u001b[0m     val_evaluator\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mval_evaluator\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    475\u001b[0m     test_evaluator\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mtest_evaluator\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    476\u001b[0m     default_hooks\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mdefault_hooks\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    477\u001b[0m     custom_hooks\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mcustom_hooks\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    478\u001b[0m     data_preprocessor\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mdata_preprocessor\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    479\u001b[0m     load_from\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mload_from\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    480\u001b[0m     resume\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mresume\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    481\u001b[0m     launcher\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mlauncher\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mnone\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    482\u001b[0m     env_cfg\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39menv_cfg\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mdict\u001b[39;49m(dist_cfg\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(backend\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mnccl\u001b[39;49m\u001b[39m'\u001b[39;49m))),\n\u001b[1;32m    483\u001b[0m     log_processor\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mlog_processor\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    484\u001b[0m     log_level\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mlog_level\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mINFO\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    485\u001b[0m     visualizer\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mvisualizer\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    486\u001b[0m     default_scope\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mdefault_scope\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmmengine\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    487\u001b[0m     randomness\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mrandomness\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mdict\u001b[39;49m(seed\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)),\n\u001b[1;32m    488\u001b[0m     experiment_name\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mexperiment_name\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    489\u001b[0m     cfg\u001b[39m=\u001b[39;49mcfg,\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    492\u001b[0m \u001b[39mreturn\u001b[39;00m runner\n",
      "File \u001b[0;32m/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmengine/runner/runner.py:309\u001b[0m, in \u001b[0;36mRunner.__init__\u001b[0;34m(self, model, work_dir, train_dataloader, val_dataloader, test_dataloader, train_cfg, val_cfg, test_cfg, auto_scale_lr, optim_wrapper, param_scheduler, val_evaluator, test_evaluator, default_hooks, custom_hooks, data_preprocessor, load_from, resume, launcher, env_cfg, log_processor, log_level, visualizer, default_scope, randomness, experiment_name, cfg)\u001b[0m\n\u001b[1;32m    306\u001b[0m training_related \u001b[39m=\u001b[39m [train_dataloader, train_cfg, optim_wrapper]\n\u001b[1;32m    307\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mall\u001b[39m(item \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m training_related)\n\u001b[1;32m    308\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mall\u001b[39m(item \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m training_related)):\n\u001b[0;32m--> 309\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    310\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtrain_dataloader, train_cfg, and optim_wrapper should be \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    311\u001b[0m         \u001b[39m'\u001b[39m\u001b[39meither all None or not None, but got \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    312\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain_dataloader=\u001b[39m\u001b[39m{\u001b[39;00mtrain_dataloader\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    313\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain_cfg=\u001b[39m\u001b[39m{\u001b[39;00mtrain_cfg\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39moptim_wrapper=\u001b[39m\u001b[39m{\u001b[39;00moptim_wrapper\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    315\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_dataloader \u001b[39m=\u001b[39m train_dataloader\n\u001b[1;32m    316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_loop \u001b[39m=\u001b[39m train_cfg\n",
      "\u001b[0;31mValueError\u001b[0m: train_dataloader, train_cfg, and optim_wrapper should be either all None or not None, but got train_dataloader={'batch_sampler': {'type': 'AspectRatioBatchSampler'}, 'batch_size': 2, 'dataset': {'ann_file': 'annotations/instances_train2017.json', 'backend_args': None, 'data_prefix': {'img': 'train2017/'}, 'data_root': '../data/coco/', 'filter_cfg': {'filter_empty_gt': True, 'min_size': 32}, 'pipeline': [{'backend_args': None, 'type': 'LoadImageFromFile'}, {'type': 'LoadAnnotations', 'with_bbox': True, 'with_mask': True}, {'keep_ratio': True, 'scale': (1333, 800), 'type': 'Resize'}, {'prob': 0.5, 'type': 'RandomFlip'}, {'type': 'PackDetInputs'}], 'type': 'CocoDataset'}, 'num_workers': 2, 'persistent_workers': True, 'sampler': {'shuffle': True, 'type': 'DefaultSampler'}}, train_cfg=None, optim_wrapper=None."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from mmengine.registry import RUNNERS\n",
    "from mmdet.utils import get_test_pipeline_cfg\n",
    "\n",
    "from mmcv.transforms import Compose\n",
    "\n",
    "cfg = model.cfg.copy()\n",
    "test_pipeline = get_test_pipeline_cfg(cfg)\n",
    "\n",
    "test_pipeline = Compose(test_pipeline)\n",
    "\n",
    "# data_ = dict(img_path='demo/demo.jpg', img_id=0)\n",
    "\n",
    "\n",
    "cfg_tr = cfg.copy()\n",
    "cfg_tr['work_dir'] = './logs'\n",
    "cfg_tr['train_dataloader']['dataset']['data_root'] = '../data/coco/'\n",
    "cfg_tr['val_dataloader']['dataset']['data_root'] = '../data/coco/'\n",
    "cfg_tr['val_evaluator']['ann_file'] = '../data/coco/annotations/instances_val2017.json'\n",
    "# print(list(cfg_tr.train_dataloader.keys()))\n",
    "runner = RUNNERS.build(cfg_tr)\n",
    "# print(cfg_tr.train_dataloader)\n",
    "# with open('data/coco/annotations/instances_train2017.json', 'r') as file:\n",
    "#     data_json = json.load(file)\n",
    "\n",
    "\n",
    "# data_ = train_pipeline(data_json)\n",
    "\n",
    "# data_['inputs'] = data_['inputs'].unsqueeze(0)\n",
    "# data_['data_samples'] = [data_['data_samples']]\n",
    "\n",
    "# print(train_pipeline)\n",
    "# print(list(data_json.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=13.15s)\n",
      "creating index...\n",
      "index created!\n",
      "1\n",
      "(tensor(0.8683, grad_fn=<AddBackward0>), OrderedDict([('loss', tensor(0.8683, grad_fn=<AddBackward0>)), ('loss_rpn_cls', tensor(0.0683, grad_fn=<AddBackward0>)), ('loss_rpn_bbox', tensor(0.0386, grad_fn=<AddBackward0>)), ('loss_cls', tensor(0.1824, grad_fn=<MeanBackward0>)), ('acc', tensor(93.9453)), ('loss_bbox', tensor(0.3257, grad_fn=<MeanBackward0>)), ('loss_mask', tensor(0.2532, grad_fn=<MeanBackward0>))]))\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dl = runner.train_dataloader\n",
    "print(model.cut_point)\n",
    "for i,data in enumerate(dl):\n",
    "    model.zero_grad()\n",
    "    model.eval()\n",
    "    # im['inputs'] = torch.stack(im['inputs'])\n",
    "    # print(list(data_.keys()))\n",
    "    # data_ = train_pipeline(im)\n",
    "    # data_ = prp(data_, False)\n",
    "    # data_['inputs'] = model.backbone.split_forward_v2(data_['inputs'], output_layer=model.cut_point-1)\n",
    "    # data_['inputs'] = {\n",
    "    #                \"hw_shape\": data_['inputs'][0],\n",
    "    #\n",
    "    #                \"outs\": data_['inputs'][1]}\n",
    "    # data = prp(data, False)\n",
    "    # print(data['inputs'][0].shape)\n",
    "    \n",
    "    feat = model.feature_frontend(data)\n",
    "    # print(feat['inputs']['outs'][0].shape)\n",
    "    print(model.backend_loss(feat))\n",
    "    print(model.cut_point)\n",
    "    # print(model.test_step(feat)[0])\n",
    "    # losses = model.loss(data_['inputs'], data_['data_samples'])\n",
    "    # loss, losses = model.parse_losses(losses)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<DetDataSample(\n",
      "\n",
      "    META INFORMATION\n",
      "    img_shape: (427, 512)\n",
      "    img_path: '../data/coco/train2017/000000578037.jpg'\n",
      "    img_id: 578037\n",
      "    ori_shape: (427, 640)\n",
      "\n",
      "    DATA FIELDS\n",
      "    ignored_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: HorizontalBoxes(\n",
      "                tensor([], size=(0, 4)))\n",
      "            labels: tensor([], dtype=torch.int64)\n",
      "            masks: BitmapMasks(num_masks=0, height=427, width=512)\n",
      "        ) at 0x7fce442ba430>\n",
      "    gt_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: HorizontalBoxes(\n",
      "                tensor([[449.9900,  25.8800, 512.0000,  70.7800],\n",
      "                        [224.1400,  61.2700, 322.7500, 382.9600],\n",
      "                        [442.5600,  83.0300, 512.0000, 316.2200],\n",
      "                        [292.3400,  91.6600, 347.4300, 265.0600],\n",
      "                        [381.0900,  91.2200, 418.5100, 188.1400],\n",
      "                        [417.5800,  86.0700, 460.8600, 205.3300],\n",
      "                        [231.5300, 358.0400, 307.2600, 394.7000],\n",
      "                        [396.9500, 106.4800, 412.1000, 140.8500],\n",
      "                        [424.0800, 112.2600, 441.9000, 148.1100],\n",
      "                        [457.0500,  81.7300, 479.0400, 122.9700],\n",
      "                        [459.5500, 115.6300, 512.0000, 160.8600]]))\n",
      "            labels: tensor([25,  0,  0,  0,  0,  0, 36, 24, 24,  0, 24])\n",
      "            masks: BitmapMasks(num_masks=11, height=427, width=512)\n",
      "        ) at 0x7fce442bafa0>\n",
      ") at 0x7fce442bafd0>, <DetDataSample(\n",
      "\n",
      "    META INFORMATION\n",
      "    img_shape: (426, 512)\n",
      "    img_path: '../data/coco/train2017/000000220471.jpg'\n",
      "    img_id: 220471\n",
      "    ori_shape: (426, 640)\n",
      "\n",
      "    DATA FIELDS\n",
      "    ignored_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: HorizontalBoxes(\n",
      "                tensor([], size=(0, 4)))\n",
      "            labels: tensor([], dtype=torch.int64)\n",
      "            masks: BitmapMasks(num_masks=0, height=426, width=512)\n",
      "        ) at 0x7fce442ba0a0>\n",
      "    gt_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: HorizontalBoxes(\n",
      "                tensor([[181.0400, 169.4400, 330.3800, 321.6500],\n",
      "                        [323.3600, 151.4900, 504.8400, 299.5400],\n",
      "                        [316.9800, 254.6400, 512.0000, 403.9800],\n",
      "                        [  0.0000, 131.7800, 129.7900, 232.9200],\n",
      "                        [407.2800,  26.2000, 512.0000, 196.1400],\n",
      "                        [138.9200,   5.7400, 251.8800, 156.0400],\n",
      "                        [  0.0000,  68.1200, 512.0000, 421.2100]]))\n",
      "            labels: tensor([48, 48, 48, 48, 41, 41, 60])\n",
      "            masks: BitmapMasks(num_masks=7, height=426, width=512)\n",
      "        ) at 0x7fce442ba7f0>\n",
      ") at 0x7fce442ba7c0>]\n"
     ]
    }
   ],
   "source": [
    "print(list(data['data_samples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0.,   0.,   0., ...,  35.,   1.,   0.],\n",
       "        [  0.,   0.,   0., ..., 100.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,  27.,   1.,   0.],\n",
       "        ...,\n",
       "        [  2.,  86.,  34., ...,  54.,   7.,   0.],\n",
       "        [  0.,   0.,   0., ...,  28.,   3.,   0.],\n",
       "        [  0.,   0.,   0., ...,  59.,   1.,   0.]]),\n",
       " array([0.        , 0.1       , 0.2       , 0.30000001, 0.40000001,\n",
       "        0.5       , 0.60000002, 0.69999999, 0.80000001, 0.89999998,\n",
       "        1.        ]),\n",
       " <a list of 2550 BarContainer objects>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "mx = feat['inputs']['outs'][0][0,:].max().item()\n",
    "mn = feat['inputs']['outs'][0][0,:].min().item()\n",
    "f_np = feat['inputs']['outs'][0][0,:].reshape(2048,-1).cpu().detach()\n",
    "f_np = f_np.detach().numpy()\n",
    "# print(f_np)\n",
    "plt.hist((f_np-mn)/(mx-mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 96, 200, 272])\n"
     ]
    }
   ],
   "source": [
    "# feat2 = feat['inputs']['outs'][0].view(-1, *feat['inputs']['hw_shape'],\n",
    "#                                 model.backbone.num_features[0]).permute(0, 3, 1,\n",
    "#                                                                 2).contiguous()\n",
    "# print(feat2.shape)\n",
    "norm0 = getattr(model.backbone, 'norm0')\n",
    "feats_to_comp  = norm0( feat['inputs']['outs'][0]).view(-1, *feat['inputs']['hw_shape'],\n",
    "                                model.backbone.num_features[0]).permute(0, 3, 1,\n",
    "                                                                2).contiguous()\n",
    "print(feats_to_comp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<DetDataSample(\n",
      "\n",
      "    META INFORMATION\n",
      "    ori_shape: (427, 640)\n",
      "    batch_input_shape: (800, 1216)\n",
      "    img_shape: (800, 1199)\n",
      "    scale_factor: (1.8734375, 1.873536299765808)\n",
      "    img_path: '../data/coco/val2017/000000397133.jpg'\n",
      "    pad_shape: (800, 1216)\n",
      "    img_id: 397133\n",
      "\n",
      "    DATA FIELDS\n",
      "    ignored_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: tensor([], size=(0, 4))\n",
      "            labels: tensor([], dtype=torch.int64)\n",
      "            masks: BitmapMasks(num_masks=0, height=427, width=640)\n",
      "        ) at 0x7fce441dbe20>\n",
      "    gt_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: tensor([[217.6200, 240.5400, 256.6100, 298.2900],\n",
      "                        [  1.0000, 240.2400, 347.6300, 427.0000],\n",
      "                        [388.6600,  69.9200, 498.0700, 347.5400],\n",
      "                        [135.5700, 249.4300, 157.8900, 278.2200],\n",
      "                        [ 31.2800, 344.0000,  99.4000, 384.8300],\n",
      "                        [ 59.6300, 287.3600, 135.7000, 328.6600],\n",
      "                        [  1.3600, 164.3300, 193.9200, 262.7000],\n",
      "                        [  0.0000, 262.8100,  62.1600, 299.5800],\n",
      "                        [119.4000, 272.5100, 144.2200, 306.7600],\n",
      "                        [141.4700, 267.9100, 173.6600, 303.7700],\n",
      "                        [155.9700, 168.9500, 182.0000, 186.0800],\n",
      "                        [157.2000, 114.1500, 175.0600, 129.9700],\n",
      "                        [ 98.7500, 304.7800, 109.5300, 310.3500],\n",
      "                        [166.0300, 256.3600, 174.8500, 274.9400],\n",
      "                        [ 86.4100, 293.9700, 110.3700, 305.1500],\n",
      "                        [ 70.1400, 296.1600,  79.4200, 300.7400],\n",
      "                        [  0.0000, 210.9000, 191.3600, 309.8800],\n",
      "                        [ 96.6900, 297.0900, 104.5300, 301.9500],\n",
      "                        [497.2500, 203.4000, 619.2600, 232.0100]])\n",
      "            labels: tensor([39, 60,  0, 43, 45, 45, 69,  0, 41, 41, 45, 45, 50, 44, 50, 50, 69, 51,\n",
      "                        71])\n",
      "            masks: BitmapMasks(num_masks=19, height=427, width=640)\n",
      "        ) at 0x7fce441dbd00>\n",
      ") at 0x7fce441dbbe0>]\n"
     ]
    }
   ],
   "source": [
    "for data in vdl:\n",
    "    # model.cut_point = 0\n",
    "    # model.data_preprocessor = model.frontend_preprocessor\n",
    "    # print(model.val_step(data))\n",
    "    # model.cut_point = 1\n",
    "    # model.data_preprocessor = TwoInputIdentity()\n",
    "    data_ = model.frontend_preprocessor(data, False)\n",
    "    print(data_['data_samples'])\n",
    "    break\n",
    "    data_['inputs'] = model.backbone.split_forward_v2(data_['inputs'], output_layer=model.cut_point-1)\n",
    "    data_['inputs'] = {#\"x\": data_['inputs'][0],\n",
    "                    \"hw_shape\": data_['inputs'][0],\n",
    "                    \"outs\": data_['inputs'][1]}\n",
    "    print(len(data_['inputs']['outs']))\n",
    "    print(data_)\n",
    "    # data_['inputs'] = [data_['inputs']]\n",
    "    # data_['data_samples'] = [data_['data_samples']]\n",
    "    # # forward the model\n",
    "    with torch.no_grad():\n",
    "        results = model.backend_inference(data_)[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.53s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.51s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "vdl = runner.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinBlockSequence(\n",
       "  (blocks): ModuleList(\n",
       "    (0): SwinBlock(\n",
       "      (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): ShiftWindowMSA(\n",
       "        (w_msa): WindowMSA(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop): DropPath()\n",
       "      )\n",
       "      (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (dropout_layer): DropPath()\n",
       "        (gamma2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): SwinBlock(\n",
       "      (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): ShiftWindowMSA(\n",
       "        (w_msa): WindowMSA(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop): DropPath()\n",
       "      )\n",
       "      (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (dropout_layer): DropPath()\n",
       "        (gamma2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (downsample): PatchMerging(\n",
       "    (adap_padding): AdaptivePadding()\n",
       "    (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))\n",
       "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backbone.stages[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b048090fc6d3014cd2bdcc9ea88274151058c974d3dd655c4d5f3eddfe0b65cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
