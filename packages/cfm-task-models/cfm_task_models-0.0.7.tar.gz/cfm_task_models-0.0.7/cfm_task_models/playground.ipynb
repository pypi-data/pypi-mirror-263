{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/21 09:14:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by local backend from path: obj_det/chkpts/mask_reppointsv2_swin_tiny_patch4_window7_3x.pth\n",
      "Done!!, save to obj_det/chkpts/mask_reppointsv2_swin_tiny_patch4_window7_3x_converted-dd7afe98.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:39: UserWarning: ``build_assigner`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_assigner`` would be deprecated soon, please use '\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:24: UserWarning: ``build_iou_calculator`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn(\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:46: UserWarning: ``build_sampler`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_sampler`` would be deprecated soon, please use '\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/apis/inference.py:90: UserWarning: dataset_meta or class names are not saved in the checkpoint's meta data, use COCO classes by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: obj_det/chkpts/mask_reppointsv2_swin_tiny_patch4_window7_3x_converted-dd7afe98.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:39: UserWarning: ``build_assigner`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_assigner`` would be deprecated soon, please use '\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:24: UserWarning: ``build_iou_calculator`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn(\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:46: UserWarning: ``build_sampler`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_sampler`` would be deprecated soon, please use '\n"
     ]
    }
   ],
   "source": [
    "# !mim download mmdet --config mask-rcnn_swin-t-p4-w7_fpn_1x_coco --dest .\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmengine.config import Config\n",
    "from cfm_task_models.legacy import *\n",
    "\n",
    "from cfm_task_models.split_utils import SplitSwinTransformer, SplitTwoStageDetector, TwoInputIdentity, SplitRepPointsV2MaskDetector\n",
    "\n",
    "config_file_old = 'obj_det/mask-rcnn_swin-t-p4-w7_fpn_1x_coco.py'\n",
    "checkpoint_file_old = 'obj_det/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth'\n",
    "config_file = 'obj_det/cfgs/mask_reppoitsv2_swin_tiny_simplified.py'\n",
    "checkpoint_file = 'obj_det/chkpts/mask_reppointsv2_swin_tiny_patch4_window7_3x.pth'\n",
    "\n",
    "\n",
    "# model_init = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n",
    "# result = inference_detector(model, 'demo/demo.jpg')\n",
    "\n",
    "# from mmdet.registry import MODELS\n",
    "# MODELS.get('SwinTransformer')\n",
    "# MODELS.get('TwoStageDetector')\n",
    "model = SplitRepPointsV2MaskDetector.create_from_cfg_and_checkpoint(config_file, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetDataPreprocessor()\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "cfg = Config.fromfile('obj_det/mask-rcnn_swin-t-p4-w7_fpn_1x_coco.py')\n",
    "model = SplitTwoStageDetector.create_from_instance_and_cfg(model_init, cfg, cut_point=1)\n",
    "\n",
    "# prp = deepcopy(model.data_preprocessor)\n",
    "# model.data_preprocessor = TwoInputIdentity()\n",
    "\n",
    "print(model.frontend_preprocessor)\n",
    "\n",
    "# model.backbone = SplitSwinTransformer.create_from_instance_and_cfg(model.backbone, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: obj_det/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TwoInputIdentity()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = SplitTwoStageDetector.create_from_cfg_and_checkpoint(config_file, checkpoint_file)\n",
    "model2.frontend_preprocessor\n",
    "model2.data_preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/13 17:54:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.5 (default, May 30 2022, 18:17:32) [GCC 7.5.0]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 1673671344\n",
      "    GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "    CUDA_HOME: /usr/local/cuda-10.1\n",
      "    NVCC: Cuda compilation tools, release 10.1, V10.1.24\n",
      "    GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
      "    PyTorch: 1.13.1+cu117\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.14.1+cu117\n",
      "    OpenCV: 4.9.0\n",
      "    MMEngine: 0.10.3\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    seed: 1673671344\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "03/13 17:54:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "auto_scale_lr = dict(base_batch_size=16, enable=False)\n",
      "backend_args = None\n",
      "data_root = 'data/coco/'\n",
      "dataset_type = 'CocoDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, type='CheckpointHook'),\n",
      "    logger=dict(interval=50, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='DetVisualizationHook'))\n",
      "default_scope = 'mmdet'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
      "max_epochs = 12\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        attn_drop_rate=0.0,\n",
      "        convert_weights=True,\n",
      "        depths=[\n",
      "            2,\n",
      "            2,\n",
      "            6,\n",
      "            2,\n",
      "        ],\n",
      "        drop_path_rate=0.2,\n",
      "        drop_rate=0.0,\n",
      "        embed_dims=96,\n",
      "        init_cfg=None,\n",
      "        mlp_ratio=4,\n",
      "        num_heads=[\n",
      "            3,\n",
      "            6,\n",
      "            12,\n",
      "            24,\n",
      "        ],\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        patch_norm=True,\n",
      "        qk_scale=None,\n",
      "        qkv_bias=True,\n",
      "        type='SwinTransformer',\n",
      "        window_size=7,\n",
      "        with_cp=False),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        pad_mask=True,\n",
      "        pad_size_divisor=32,\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='DetDataPreprocessor'),\n",
      "    neck=dict(\n",
      "        in_channels=[\n",
      "            96,\n",
      "            192,\n",
      "            384,\n",
      "            768,\n",
      "        ],\n",
      "        num_outs=5,\n",
      "        out_channels=256,\n",
      "        type='FPN'),\n",
      "    roi_head=dict(\n",
      "        bbox_head=dict(\n",
      "            bbox_coder=dict(\n",
      "                target_means=[\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                ],\n",
      "                target_stds=[\n",
      "                    0.1,\n",
      "                    0.1,\n",
      "                    0.2,\n",
      "                    0.2,\n",
      "                ],\n",
      "                type='DeltaXYWHBBoxCoder'),\n",
      "            fc_out_channels=1024,\n",
      "            in_channels=256,\n",
      "            loss_bbox=dict(loss_weight=1.0, type='L1Loss'),\n",
      "            loss_cls=dict(\n",
      "                loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=False),\n",
      "            num_classes=80,\n",
      "            reg_class_agnostic=False,\n",
      "            roi_feat_size=7,\n",
      "            type='Shared2FCBBoxHead'),\n",
      "        bbox_roi_extractor=dict(\n",
      "            featmap_strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ],\n",
      "            out_channels=256,\n",
      "            roi_layer=dict(output_size=7, sampling_ratio=0, type='RoIAlign'),\n",
      "            type='SingleRoIExtractor'),\n",
      "        mask_head=dict(\n",
      "            conv_out_channels=256,\n",
      "            in_channels=256,\n",
      "            loss_mask=dict(\n",
      "                loss_weight=1.0, type='CrossEntropyLoss', use_mask=True),\n",
      "            num_classes=80,\n",
      "            num_convs=4,\n",
      "            type='FCNMaskHead'),\n",
      "        mask_roi_extractor=dict(\n",
      "            featmap_strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ],\n",
      "            out_channels=256,\n",
      "            roi_layer=dict(output_size=14, sampling_ratio=0, type='RoIAlign'),\n",
      "            type='SingleRoIExtractor'),\n",
      "        test_cfg=dict(\n",
      "            mask_thr_binary=0.5,\n",
      "            max_per_img=100,\n",
      "            nms=dict(iou_threshold=0.5, type='nms'),\n",
      "            score_thr=0.05),\n",
      "        train_cfg=dict(\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=True,\n",
      "                min_pos_iou=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                pos_iou_thr=0.5,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            mask_size=28,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=True,\n",
      "                neg_pos_ub=-1,\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                type='RandomSampler')),\n",
      "        type='StandardRoIHead'),\n",
      "    rpn_head=dict(\n",
      "        anchor_generator=dict(\n",
      "            ratios=[\n",
      "                0.5,\n",
      "                1.0,\n",
      "                2.0,\n",
      "            ],\n",
      "            scales=[\n",
      "                8,\n",
      "            ],\n",
      "            strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "                64,\n",
      "            ],\n",
      "            type='AnchorGenerator'),\n",
      "        bbox_coder=dict(\n",
      "            target_means=[\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "            ],\n",
      "            target_stds=[\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "            ],\n",
      "            type='DeltaXYWHBBoxCoder'),\n",
      "        feat_channels=256,\n",
      "        in_channels=256,\n",
      "        loss_bbox=dict(loss_weight=1.0, type='L1Loss'),\n",
      "        loss_cls=dict(\n",
      "            loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=True),\n",
      "        type='RPNHead'),\n",
      "    test_cfg=dict(\n",
      "        rcnn=dict(\n",
      "            mask_thr_binary=0.5,\n",
      "            max_per_img=100,\n",
      "            nms=dict(iou_threshold=0.5, type='nms'),\n",
      "            score_thr=0.05),\n",
      "        rpn=dict(\n",
      "            max_per_img=1000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=1000)),\n",
      "    train_cfg=dict(\n",
      "        rcnn=dict(\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=True,\n",
      "                min_pos_iou=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                pos_iou_thr=0.5,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            mask_size=28,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=True,\n",
      "                neg_pos_ub=-1,\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                type='RandomSampler')),\n",
      "        rpn=dict(\n",
      "            allowed_border=-1,\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=True,\n",
      "                min_pos_iou=0.3,\n",
      "                neg_iou_thr=0.3,\n",
      "                pos_iou_thr=0.7,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=False,\n",
      "                neg_pos_ub=-1,\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                type='RandomSampler')),\n",
      "        rpn_proposal=dict(\n",
      "            max_per_img=1000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=2000)),\n",
      "    type='MaskRCNN')\n",
      "optim_wrapper = dict(\n",
      "    optimizer=dict(\n",
      "        betas=(\n",
      "            0.9,\n",
      "            0.999,\n",
      "        ), lr=0.0001, type='AdamW', weight_decay=0.05),\n",
      "    paramwise_cfg=dict(\n",
      "        custom_keys=dict(\n",
      "            absolute_pos_embed=dict(decay_mult=0.0),\n",
      "            norm=dict(decay_mult=0.0),\n",
      "            relative_position_bias_table=dict(decay_mult=0.0))),\n",
      "    type='OptimWrapper')\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0, by_epoch=False, end=1000, start_factor=0.001,\n",
      "        type='LinearLR'),\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        end=12,\n",
      "        gamma=0.1,\n",
      "        milestones=[\n",
      "            8,\n",
      "            11,\n",
      "        ],\n",
      "        type='MultiStepLR'),\n",
      "]\n",
      "pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth'\n",
      "resume = False\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_val2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='val2017/'),\n",
      "        data_root='data/coco/',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    ann_file='data/coco/annotations/instances_val2017.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric=[\n",
      "        'bbox',\n",
      "        'segm',\n",
      "    ],\n",
      "    type='CocoMetric')\n",
      "test_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "        ),\n",
      "        type='PackDetInputs'),\n",
      "]\n",
      "train_cfg = dict(max_epochs=12, type='EpochBasedTrainLoop', val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_train2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='train2017/'),\n",
      "        data_root='../data/coco/',\n",
      "        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(crop_size=(\n",
      "                512,\n",
      "                512,\n",
      "            ), type='RandomCrop'),\n",
      "            dict(prob=0.5, type='RandomFlip'),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        type='CocoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackDetInputs'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='annotations/instances_val2017.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='val2017/'),\n",
      "        data_root='../data/coco/',\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                1333,\n",
      "                800,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    ann_file='../data/coco/annotations/instances_val2017.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric=[\n",
      "        'bbox',\n",
      "        'segm',\n",
      "    ],\n",
      "    type='CocoMetric')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='DetLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = './logs'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmengine/utils/manager.py:113: UserWarning: <class 'mmdet.visualization.local_visualizer.DetLocalVisualizer'> instance named of visualizer has been created, the method `get_instance` should not accept any other arguments\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/13 17:54:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "03/13 17:54:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from mmengine.registry import RUNNERS\n",
    "from mmdet.utils import get_test_pipeline_cfg\n",
    "\n",
    "from mmcv.transforms import Compose\n",
    "\n",
    "cfg = model_init.cfg.copy()\n",
    "test_pipeline = get_test_pipeline_cfg(cfg)\n",
    "\n",
    "test_pipeline = Compose(test_pipeline)\n",
    "\n",
    "# data_ = dict(img_path='demo/demo.jpg', img_id=0)\n",
    "\n",
    "\n",
    "cfg_tr = cfg.copy()\n",
    "cfg_tr['work_dir'] = './logs'\n",
    "cfg_tr['train_dataloader']['dataset']['data_root'] = '../data/coco/'\n",
    "cfg_tr['val_dataloader']['dataset']['data_root'] = '../data/coco/'\n",
    "cfg_tr['val_evaluator']['ann_file'] = '../data/coco/annotations/instances_val2017.json'\n",
    "# print(list(cfg_tr.train_dataloader.keys()))\n",
    "runner = RUNNERS.build(cfg_tr)\n",
    "# print(cfg_tr.train_dataloader)\n",
    "# with open('data/coco/annotations/instances_train2017.json', 'r') as file:\n",
    "#     data_json = json.load(file)\n",
    "\n",
    "\n",
    "# data_ = train_pipeline(data_json)\n",
    "\n",
    "# data_['inputs'] = data_['inputs'].unsqueeze(0)\n",
    "# data_['data_samples'] = [data_['data_samples']]\n",
    "\n",
    "# print(train_pipeline)\n",
    "# print(list(data_json.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=13.15s)\n",
      "creating index...\n",
      "index created!\n",
      "1\n",
      "(tensor(0.8683, grad_fn=<AddBackward0>), OrderedDict([('loss', tensor(0.8683, grad_fn=<AddBackward0>)), ('loss_rpn_cls', tensor(0.0683, grad_fn=<AddBackward0>)), ('loss_rpn_bbox', tensor(0.0386, grad_fn=<AddBackward0>)), ('loss_cls', tensor(0.1824, grad_fn=<MeanBackward0>)), ('acc', tensor(93.9453)), ('loss_bbox', tensor(0.3257, grad_fn=<MeanBackward0>)), ('loss_mask', tensor(0.2532, grad_fn=<MeanBackward0>))]))\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dl = runner.train_dataloader\n",
    "print(model.cut_point)\n",
    "for i,data in enumerate(dl):\n",
    "    model.zero_grad()\n",
    "    model.eval()\n",
    "    # im['inputs'] = torch.stack(im['inputs'])\n",
    "    # print(list(data_.keys()))\n",
    "    # data_ = train_pipeline(im)\n",
    "    # data_ = prp(data_, False)\n",
    "    # data_['inputs'] = model.backbone.split_forward_v2(data_['inputs'], output_layer=model.cut_point-1)\n",
    "    # data_['inputs'] = {\n",
    "    #                \"hw_shape\": data_['inputs'][0],\n",
    "    #\n",
    "    #                \"outs\": data_['inputs'][1]}\n",
    "    # data = prp(data, False)\n",
    "    # print(data['inputs'][0].shape)\n",
    "    \n",
    "    feat = model.feature_frontend(data)\n",
    "    # print(feat['inputs']['outs'][0].shape)\n",
    "    print(model.backend_loss(feat))\n",
    "    print(model.cut_point)\n",
    "    # print(model.test_step(feat)[0])\n",
    "    # losses = model.loss(data_['inputs'], data_['data_samples'])\n",
    "    # loss, losses = model.parse_losses(losses)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<DetDataSample(\n",
      "\n",
      "    META INFORMATION\n",
      "    img_shape: (427, 512)\n",
      "    img_path: '../data/coco/train2017/000000578037.jpg'\n",
      "    img_id: 578037\n",
      "    ori_shape: (427, 640)\n",
      "\n",
      "    DATA FIELDS\n",
      "    ignored_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: HorizontalBoxes(\n",
      "                tensor([], size=(0, 4)))\n",
      "            labels: tensor([], dtype=torch.int64)\n",
      "            masks: BitmapMasks(num_masks=0, height=427, width=512)\n",
      "        ) at 0x7fce442ba430>\n",
      "    gt_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: HorizontalBoxes(\n",
      "                tensor([[449.9900,  25.8800, 512.0000,  70.7800],\n",
      "                        [224.1400,  61.2700, 322.7500, 382.9600],\n",
      "                        [442.5600,  83.0300, 512.0000, 316.2200],\n",
      "                        [292.3400,  91.6600, 347.4300, 265.0600],\n",
      "                        [381.0900,  91.2200, 418.5100, 188.1400],\n",
      "                        [417.5800,  86.0700, 460.8600, 205.3300],\n",
      "                        [231.5300, 358.0400, 307.2600, 394.7000],\n",
      "                        [396.9500, 106.4800, 412.1000, 140.8500],\n",
      "                        [424.0800, 112.2600, 441.9000, 148.1100],\n",
      "                        [457.0500,  81.7300, 479.0400, 122.9700],\n",
      "                        [459.5500, 115.6300, 512.0000, 160.8600]]))\n",
      "            labels: tensor([25,  0,  0,  0,  0,  0, 36, 24, 24,  0, 24])\n",
      "            masks: BitmapMasks(num_masks=11, height=427, width=512)\n",
      "        ) at 0x7fce442bafa0>\n",
      ") at 0x7fce442bafd0>, <DetDataSample(\n",
      "\n",
      "    META INFORMATION\n",
      "    img_shape: (426, 512)\n",
      "    img_path: '../data/coco/train2017/000000220471.jpg'\n",
      "    img_id: 220471\n",
      "    ori_shape: (426, 640)\n",
      "\n",
      "    DATA FIELDS\n",
      "    ignored_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: HorizontalBoxes(\n",
      "                tensor([], size=(0, 4)))\n",
      "            labels: tensor([], dtype=torch.int64)\n",
      "            masks: BitmapMasks(num_masks=0, height=426, width=512)\n",
      "        ) at 0x7fce442ba0a0>\n",
      "    gt_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: HorizontalBoxes(\n",
      "                tensor([[181.0400, 169.4400, 330.3800, 321.6500],\n",
      "                        [323.3600, 151.4900, 504.8400, 299.5400],\n",
      "                        [316.9800, 254.6400, 512.0000, 403.9800],\n",
      "                        [  0.0000, 131.7800, 129.7900, 232.9200],\n",
      "                        [407.2800,  26.2000, 512.0000, 196.1400],\n",
      "                        [138.9200,   5.7400, 251.8800, 156.0400],\n",
      "                        [  0.0000,  68.1200, 512.0000, 421.2100]]))\n",
      "            labels: tensor([48, 48, 48, 48, 41, 41, 60])\n",
      "            masks: BitmapMasks(num_masks=7, height=426, width=512)\n",
      "        ) at 0x7fce442ba7f0>\n",
      ") at 0x7fce442ba7c0>]\n"
     ]
    }
   ],
   "source": [
    "print(list(data['data_samples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0.,   0.,   0., ...,  35.,   1.,   0.],\n",
       "        [  0.,   0.,   0., ..., 100.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,  27.,   1.,   0.],\n",
       "        ...,\n",
       "        [  2.,  86.,  34., ...,  54.,   7.,   0.],\n",
       "        [  0.,   0.,   0., ...,  28.,   3.,   0.],\n",
       "        [  0.,   0.,   0., ...,  59.,   1.,   0.]]),\n",
       " array([0.        , 0.1       , 0.2       , 0.30000001, 0.40000001,\n",
       "        0.5       , 0.60000002, 0.69999999, 0.80000001, 0.89999998,\n",
       "        1.        ]),\n",
       " <a list of 2550 BarContainer objects>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "mx = feat['inputs']['outs'][0][0,:].max().item()\n",
    "mn = feat['inputs']['outs'][0][0,:].min().item()\n",
    "f_np = feat['inputs']['outs'][0][0,:].reshape(2048,-1).cpu().detach()\n",
    "f_np = f_np.detach().numpy()\n",
    "# print(f_np)\n",
    "plt.hist((f_np-mn)/(mx-mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 96, 200, 272])\n"
     ]
    }
   ],
   "source": [
    "# feat2 = feat['inputs']['outs'][0].view(-1, *feat['inputs']['hw_shape'],\n",
    "#                                 model.backbone.num_features[0]).permute(0, 3, 1,\n",
    "#                                                                 2).contiguous()\n",
    "# print(feat2.shape)\n",
    "norm0 = getattr(model.backbone, 'norm0')\n",
    "feats_to_comp  = norm0( feat['inputs']['outs'][0]).view(-1, *feat['inputs']['hw_shape'],\n",
    "                                model.backbone.num_features[0]).permute(0, 3, 1,\n",
    "                                                                2).contiguous()\n",
    "print(feats_to_comp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<DetDataSample(\n",
      "\n",
      "    META INFORMATION\n",
      "    ori_shape: (427, 640)\n",
      "    batch_input_shape: (800, 1216)\n",
      "    img_shape: (800, 1199)\n",
      "    scale_factor: (1.8734375, 1.873536299765808)\n",
      "    img_path: '../data/coco/val2017/000000397133.jpg'\n",
      "    pad_shape: (800, 1216)\n",
      "    img_id: 397133\n",
      "\n",
      "    DATA FIELDS\n",
      "    ignored_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: tensor([], size=(0, 4))\n",
      "            labels: tensor([], dtype=torch.int64)\n",
      "            masks: BitmapMasks(num_masks=0, height=427, width=640)\n",
      "        ) at 0x7fce441dbe20>\n",
      "    gt_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: tensor([[217.6200, 240.5400, 256.6100, 298.2900],\n",
      "                        [  1.0000, 240.2400, 347.6300, 427.0000],\n",
      "                        [388.6600,  69.9200, 498.0700, 347.5400],\n",
      "                        [135.5700, 249.4300, 157.8900, 278.2200],\n",
      "                        [ 31.2800, 344.0000,  99.4000, 384.8300],\n",
      "                        [ 59.6300, 287.3600, 135.7000, 328.6600],\n",
      "                        [  1.3600, 164.3300, 193.9200, 262.7000],\n",
      "                        [  0.0000, 262.8100,  62.1600, 299.5800],\n",
      "                        [119.4000, 272.5100, 144.2200, 306.7600],\n",
      "                        [141.4700, 267.9100, 173.6600, 303.7700],\n",
      "                        [155.9700, 168.9500, 182.0000, 186.0800],\n",
      "                        [157.2000, 114.1500, 175.0600, 129.9700],\n",
      "                        [ 98.7500, 304.7800, 109.5300, 310.3500],\n",
      "                        [166.0300, 256.3600, 174.8500, 274.9400],\n",
      "                        [ 86.4100, 293.9700, 110.3700, 305.1500],\n",
      "                        [ 70.1400, 296.1600,  79.4200, 300.7400],\n",
      "                        [  0.0000, 210.9000, 191.3600, 309.8800],\n",
      "                        [ 96.6900, 297.0900, 104.5300, 301.9500],\n",
      "                        [497.2500, 203.4000, 619.2600, 232.0100]])\n",
      "            labels: tensor([39, 60,  0, 43, 45, 45, 69,  0, 41, 41, 45, 45, 50, 44, 50, 50, 69, 51,\n",
      "                        71])\n",
      "            masks: BitmapMasks(num_masks=19, height=427, width=640)\n",
      "        ) at 0x7fce441dbd00>\n",
      ") at 0x7fce441dbbe0>]\n"
     ]
    }
   ],
   "source": [
    "for data in vdl:\n",
    "    # model.cut_point = 0\n",
    "    # model.data_preprocessor = model.frontend_preprocessor\n",
    "    # print(model.val_step(data))\n",
    "    # model.cut_point = 1\n",
    "    # model.data_preprocessor = TwoInputIdentity()\n",
    "    data_ = model.frontend_preprocessor(data, False)\n",
    "    print(data_['data_samples'])\n",
    "    break\n",
    "    data_['inputs'] = model.backbone.split_forward_v2(data_['inputs'], output_layer=model.cut_point-1)\n",
    "    data_['inputs'] = {#\"x\": data_['inputs'][0],\n",
    "                    \"hw_shape\": data_['inputs'][0],\n",
    "                    \"outs\": data_['inputs'][1]}\n",
    "    print(len(data_['inputs']['outs']))\n",
    "    print(data_)\n",
    "    # data_['inputs'] = [data_['inputs']]\n",
    "    # data_['data_samples'] = [data_['data_samples']]\n",
    "    # # forward the model\n",
    "    with torch.no_grad():\n",
    "        results = model.backend_inference(data_)[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.53s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.51s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "vdl = runner.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinBlockSequence(\n",
       "  (blocks): ModuleList(\n",
       "    (0): SwinBlock(\n",
       "      (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): ShiftWindowMSA(\n",
       "        (w_msa): WindowMSA(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop): DropPath()\n",
       "      )\n",
       "      (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (dropout_layer): DropPath()\n",
       "        (gamma2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): SwinBlock(\n",
       "      (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): ShiftWindowMSA(\n",
       "        (w_msa): WindowMSA(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "        (drop): DropPath()\n",
       "      )\n",
       "      (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (dropout_layer): DropPath()\n",
       "        (gamma2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (downsample): PatchMerging(\n",
       "    (adap_padding): AdaptivePadding()\n",
       "    (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))\n",
       "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backbone.stages[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b048090fc6d3014cd2bdcc9ea88274151058c974d3dd655c4d5f3eddfe0b65cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
