{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with uploading your data to the atlas website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/int-brain-lab/iblbrainviewer/blob/main/docs/atlas_website_getting_started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website [http://atlas.internationalbrainlab.org](http://atlas.internationalbrainlab.org) enables you to upload and region, volumetric and point data and explore it and in interactive way using different anatomical views.\n",
    "\n",
    "This notebook walks you through how you can prepare some region based data and shows you how can upload it onto the website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install the necessary packages on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install iblbrainviewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we show how to upload region based data. For this type of data you will have to declare:\n",
    "\n",
    "- A `regions` array containing the brain regions to be displayed. The regions can be given either as Allen `acronyms` or `atlas_ids`\n",
    "- One (or more) `feature` array(s) containing the values to be displayed, each corresponding to a brain region listed in the `regions` array\n",
    "- The names of each features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of possible brain regions contained in the Allen structure tree can be explored in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1328 acronyms; first 10 are named: ['6b' 'A13' 'AAA' 'ACA' 'ACA1' 'ACA2/3' 'ACA5' 'ACA6a' 'ACA6b' 'ACAd'] ...\n"
     ]
    }
   ],
   "source": [
    "from iblatlas.regions import BrainRegions\n",
    "import numpy as np\n",
    "\n",
    "br = BrainRegions()\n",
    "\n",
    "acronyms = np.unique(br.acronym)\n",
    "\n",
    "# Print the list of brain regions (first 100)\n",
    "print(f'There are {len(acronyms)} acronyms; first 10 are named: {acronyms[0:10]} ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine you want to plot 3 features (e.g. the mean spike rate, amplitude, or LFP phase), for the following 4 brain regions : MOp1, MOs5, AAA and ACAd2/3. The arrays and associated variables would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acronyms = np.array(['MOp1', 'MOs5', 'AAA', 'ACAd2/3'])\n",
    "\n",
    "# Spike rate features\n",
    "fname1 = 'spike_rate'\n",
    "values1 = np.array([1.2, 3.4, 0.0, 0.1])\n",
    "\n",
    "# Spike amplitude feature\n",
    "fname2 = 'spike_amplitude'\n",
    "values2 = np.array([33.5, 234.2, 5.3, 10.6])\n",
    "\n",
    "# LFP phase feature\n",
    "fname3 = 'lfp_phase'\n",
    "values3 = np.array([36.0, 12.3, 34.0, 56.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that the regions and features are the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the arrays have the same length\n",
    "assert acronyms.size == values1.size\n",
    "assert acronyms.size == values2.size\n",
    "assert acronyms.size == values3.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understand some useful concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we upload our example data to the website, we will introduce you to some useful concpets that underly how the data you upload is structured and organised.\n",
    "\n",
    "\n",
    "### Bucket\n",
    "All data are organised into buckets. A bucket acts like a folder and contains all data common to a given project or user. A single bucket can contain multiple features.\n",
    "\n",
    "### Local Bucket\n",
    "A local bucket is a folder on your local filesystem. Features can be added to the local bucket and uploaded to the website. The data is stored in the browser cache and will persist until the browser cache is cleared. All users have to their own unique local bucket and the data can only be accessed by the local user.\n",
    "\n",
    "### Remote Bucket\n",
    "A remote bucket is a folder that can be created on a remote server. When adding data to a remote bucket, the data is uploaded to our server and then shown on the website. This data will always be available (see storage terms and conditions) and can be viewed by multiple users.\n",
    "\n",
    "### Local vs remote bucket\n",
    "The main difference between data stored on a local vs a remote bucket is the ability for the data to persist and to be shared with others. \n",
    "\n",
    "### Features\n",
    "A feature is the data to be displayed. A feature can be \n",
    "\n",
    "- Region based - a map of individual brain regions to values (e.g `spike_rate` or  `spike_amplitude` defined above)\n",
    "- Volume based - full 3D volume\n",
    "- Point based - 3D coordinates of individual points (e.g clusters or channels)\n",
    "\n",
    "A bucket can contain many features. The name of each feature within a bucket must be unique\n",
    "\n",
    "### Tree\n",
    "A tree defines the organistion of features within a bucket. Features can be arranged into folders within folders or renamed for display purposes. Trees can only be applied to remote buckets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload your data to the website via a local bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature uploader API is used to upload your data to the `atlas` website. To start uploading your data to the local bucket, import the feature uploader and instantiate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iblbrainviewer.api import FeatureUploader\n",
    "\n",
    "up = FeatureUploader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the bucket name is `local`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "print(up.bucket_uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a path on our local computer that will store our local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/admin/local_bucket\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "bucket_path = Path.home().joinpath('local_bucket')\n",
    "bucket_path.mkdir(exist_ok=True)\n",
    "print(bucket_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can upload our region based features to our bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "up.local_features(fname1, acronyms, values1, hemisphere='left', output_dir=bucket_path)\n",
    "up.local_features(fname2, acronyms, values2, hemisphere='left', output_dir=bucket_path)\n",
    "up.local_features(fname3, acronyms, values3, hemisphere='left', output_dir=bucket_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you now navigate to the location of `bucket_path` on your local file system you should see the three features. \n",
    " - lfp_phase.json\n",
    " - spike_amplitude.json\n",
    " - spike_rate.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload these features to the website, select the local bucket from the drop down menu on the top left hand side of the webpage and click on the up arrow button and choose a feature or multiple features to upload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload your data to the website via a remote bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload your data to a remote bucket you will first need to define bucket name.  The name of the bucket should ideally easily identify what your bucket will contain, or who it belongs to. You can follow the convention `my3initials__myproject`, e.g. `sebcha_serotonin`. We can then import the FeatureUploader and this time instantiate it with the bucket name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iblbrainviewer.api import FeatureUploader\n",
    "\n",
    "bucket = 'ibl_getting_started'\n",
    "# Create or load the bucket\n",
    "up = FeatureUploader(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time you create a bucket you will be prompted to enter a global key. The global key is `f9134149-fcb5-4142-a0eb-4e76a0811cf9`. Once the FeatureUploader is instantiated you can upload your features to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features.\n",
    "if not up.features_exist(fname1):\n",
    "    up.create_features(fname1, acronyms, values1, hemisphere='left')\n",
    "if not up.features_exist(fname2):\n",
    "    up.create_features(fname2, acronyms, values2, hemisphere='left')\n",
    "if not up.features_exist(fname3):\n",
    "    up.create_features(fname3, acronyms, values3, hemisphere='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have uploaded your features you can then generate a url to your bucket in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://atlas.internationalbrainlab.org/?buckets=ibl_getting_started&bucket=ibl_getting_started\n"
     ]
    }
   ],
   "source": [
    "url = up.get_buckets_url([bucket])\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you follow this link you should see these example features in the dropdown tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a tree\n",
    "A tree can be added to a remote bucket to define the organistaion of the features in your bucket. By default, when a bucket is created it does not contain a tree. On the display all the features will be named by their variable names `fname`, and on the same hierarchy level (i.e. as one drop-down list).\n",
    "\n",
    "If you want to organise your features into groups, or change the naming of the features displayed you can define a tree. \n",
    "\n",
    "The tree allows you to\n",
    "- Hierachically organise the features in your bucket\n",
    "- Change the name displayed for each feature on the webpage\n",
    "- Exclude the display of chosen features\n",
    "\n",
    "For example here, we will collate the spikes feature into a group called \"Spike\" and also rename the feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tree = {'Spike': {'mean spike rate (Hz)': fname1,\n",
    "                  'mean spike amplitude (uV)': fname2},\n",
    "        'LFP phase (8-10Hz band) (rad)': fname3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the tree to the bucket in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://atlas.internationalbrainlab.org/?buckets=ibl_getting_started&bucket=ibl_getting_started\n"
     ]
    }
   ],
   "source": [
    "up.patch_bucket(tree=tree)\n",
    "url = up.get_buckets_url([bucket])\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you open the url again and hit the refresh button that you see that at the top centre of the box showing your features list, you should find that the features have been rearranaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sharing your data with others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The url generated above can be shared with others and they will be able to view the data in your remote bucket. Alternatively, if the unique name of the remote bucket is known, the `+` button on the left hand side of the website can be clicked, the name entered and the bucket loaded. \n",
    "\n",
    "A unique token is automatically generated upon creation of a new remote bucket and stored locally on the users computer. This ensures that while the remote bucket can viewed by anyone with the url or the bucket name, only people with access to this unique token can upload, change or delete data contained within the remote bucket. Please refer to the **Instantiating a bucket with a token** section in the [API tutorial](https://github.com/int-brain-lab/iblbrainviewer/blob/main/docs/atlas_website_using_the_API.ipynb) for an example of how to share the bucket token between users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Read more preparing and uploading region data to the website](https://github.com/int-brain-lab/iblbrainviewer/blob/main/docs/atlas_website_preparing_region_data.ipynb)\n",
    "- [Read more preparing and uploading volumetric data to the website](https://github.com/int-brain-lab/iblbrainviewer/blob/main/docs/atlas_website_preparing_volumetric_data.ipynb)\n",
    "- [Read more preparing and uploading point data to the website](https://github.com/int-brain-lab/iblbrainviewer/blob/main/docs/atlas_website_preparing_point_data.ipynb)\n",
    "- [Read more about the different mappings used to display the data](https://github.com/int-brain-lab/iblbrainviewer/blob/main/docs/atlas_website_understanding_the_mappings.ipynb)\n",
    "- [Get familiar with the FeatureUploader API](https://github.com/int-brain-lab/iblbrainviewer/blob/main/docs/atlas_website_using_the_API.ipynb)\n",
    "- [Get familiar with using the website](https://github.com/int-brain-lab/iblbrainviewer/blob/main/docs/atlas_website_using_the_website.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
